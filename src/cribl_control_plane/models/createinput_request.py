"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authenticationmethodoptions import AuthenticationMethodOptions
from .authenticationmethodoptions1 import AuthenticationMethodOptions1
from .authenticationmethodoptionsauthtokensitems import (
    AuthenticationMethodOptionsAuthTokensItems,
)
from .authenticationmethodoptionss3collectorconf import (
    AuthenticationMethodOptionsS3CollectorConf,
)
from .authenticationmethodoptionssasl import AuthenticationMethodOptionsSasl
from .authenticationtype import AuthenticationType, AuthenticationTypeTypedDict
from .authenticationtype1 import AuthenticationType1, AuthenticationType1TypedDict
from .authenticationtypeoptionslokiauth import AuthenticationTypeOptionsLokiAuth
from .authenticationtypeoptionsprometheusauth import (
    AuthenticationTypeOptionsPrometheusAuth,
)
from .certificatetypeazureblobauthtypeclientcert import (
    CertificateTypeAzureBlobAuthTypeClientCert,
    CertificateTypeAzureBlobAuthTypeClientCertTypedDict,
)
from .createinput_inputcribllakehttp import (
    CreateInputInputAppscope,
    CreateInputInputAppscopeTypedDict,
    CreateInputInputCloudflareHec,
    CreateInputInputCloudflareHecTypedDict,
    CreateInputInputCriblLakeHTTP,
    CreateInputInputCriblLakeHTTPTypedDict,
    CreateInputInputCriblmetrics,
    CreateInputInputCriblmetricsTypedDict,
    CreateInputInputCrowdstrike,
    CreateInputInputCrowdstrikeTypedDict,
    CreateInputInputDatadogAgent,
    CreateInputInputDatadogAgentTypedDict,
    CreateInputInputDatagen,
    CreateInputInputDatagenTypedDict,
    CreateInputInputFile,
    CreateInputInputFileTypedDict,
    CreateInputInputHTTPRaw,
    CreateInputInputHTTPRawTypedDict,
    CreateInputInputJournalFiles,
    CreateInputInputJournalFilesTypedDict,
    CreateInputInputKinesis,
    CreateInputInputKinesisTypedDict,
    CreateInputInputKubeEvents,
    CreateInputInputKubeEventsTypedDict,
    CreateInputInputKubeLogs,
    CreateInputInputKubeLogsTypedDict,
    CreateInputInputKubeMetrics,
    CreateInputInputKubeMetricsTypedDict,
    CreateInputInputMetrics,
    CreateInputInputMetricsTypedDict,
    CreateInputInputModelDrivenTelemetry,
    CreateInputInputModelDrivenTelemetryTypedDict,
    CreateInputInputNetflow,
    CreateInputInputNetflowTypedDict,
    CreateInputInputOpenTelemetry,
    CreateInputInputOpenTelemetryTypedDict,
    CreateInputInputRawUDP,
    CreateInputInputRawUDPTypedDict,
    CreateInputInputS3,
    CreateInputInputS3Inventory,
    CreateInputInputS3InventoryTypedDict,
    CreateInputInputS3TypedDict,
    CreateInputInputSecurityLake,
    CreateInputInputSecurityLakeTypedDict,
    CreateInputInputSnmp,
    CreateInputInputSnmpTypedDict,
    CreateInputInputSqs,
    CreateInputInputSqsTypedDict,
    CreateInputInputSyslogUnion,
    CreateInputInputSyslogUnionTypedDict,
    CreateInputInputSystemMetrics,
    CreateInputInputSystemMetricsTypedDict,
    CreateInputInputSystemState,
    CreateInputInputSystemStateTypedDict,
    CreateInputInputTCP,
    CreateInputInputTCPTypedDict,
    CreateInputInputTcpjson,
    CreateInputInputTcpjsonTypedDict,
    CreateInputInputWef,
    CreateInputInputWefTypedDict,
    CreateInputInputWinEventLogs,
    CreateInputInputWinEventLogsTypedDict,
    CreateInputInputWindowsMetrics,
    CreateInputInputWindowsMetricsTypedDict,
    CreateInputInputWiz,
    CreateInputInputWizTypedDict,
    CreateInputInputWizWebhook,
    CreateInputInputWizWebhookTypedDict,
    CreateInputInputZscalerHec,
    CreateInputInputZscalerHecTypedDict,
)
from .diskspoolingtype import DiskSpoolingType, DiskSpoolingTypeTypedDict
from .googleauthenticationmethodoptions import GoogleAuthenticationMethodOptions
from .itemstypeauthtokens import ItemsTypeAuthTokens, ItemsTypeAuthTokensTypedDict
from .itemstypeauthtokensext import (
    ItemsTypeAuthTokensExt,
    ItemsTypeAuthTokensExtTypedDict,
)
from .itemstypeconnectionsoptional import (
    ItemsTypeConnectionsOptional,
    ItemsTypeConnectionsOptionalTypedDict,
)
from .itemstypeextrahttpheaders import (
    ItemsTypeExtraHTTPHeaders,
    ItemsTypeExtraHTTPHeadersTypedDict,
)
from .itemstypenotificationmetadata import (
    ItemsTypeNotificationMetadata,
    ItemsTypeNotificationMetadataTypedDict,
)
from .itemstypeoauthheaders import ItemsTypeOauthHeaders, ItemsTypeOauthHeadersTypedDict
from .itemstypeoauthparams import ItemsTypeOauthParams, ItemsTypeOauthParamsTypedDict
from .itemstypesearchfilter import ItemsTypeSearchFilter, ItemsTypeSearchFilterTypedDict
from .kafkaschemaregistryauthenticationtype import (
    KafkaSchemaRegistryAuthenticationType,
    KafkaSchemaRegistryAuthenticationTypeTypedDict,
)
from .logleveloptionscontentconfigitems import LogLevelOptionsContentConfigItems
from .outputmodeoptionssplunkcollectorconf import OutputModeOptionsSplunkCollectorConf
from .pqtype import PqType, PqTypeTypedDict
from .preprocesstypesavedjobcollectioninput import (
    PreprocessTypeSavedJobCollectionInput,
    PreprocessTypeSavedJobCollectionInputTypedDict,
)
from .protocoloptionstargetsitems import ProtocolOptionsTargetsItems
from .recordtypeoptions import RecordTypeOptions
from .retryrulestype import RetryRulesType, RetryRulesTypeTypedDict
from .retryrulestype1 import RetryRulesType1, RetryRulesType1TypedDict
from .signatureversionoptions import SignatureVersionOptions
from .signatureversionoptions1 import SignatureVersionOptions1
from .subscriptionplanoptions import SubscriptionPlanOptions
from .tlssettingsclientsidetype import (
    TLSSettingsClientSideType,
    TLSSettingsClientSideTypeTypedDict,
)
from .tlssettingsclientsidetypekafkaschemaregistry import (
    TLSSettingsClientSideTypeKafkaSchemaRegistry,
    TLSSettingsClientSideTypeKafkaSchemaRegistryTypedDict,
)
from .tlssettingsserversidetype import (
    TLSSettingsServerSideType,
    TLSSettingsServerSideTypeTypedDict,
)
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from cribl_control_plane.utils import get_discriminator
from enum import Enum
import pydantic
from pydantic import Discriminator, Tag, field_serializer, model_serializer
from typing import Any, List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class CreateInputTypeCriblHTTP(str, Enum):
    CRIBL_HTTP = "cribl_http"


class CreateInputInputCriblHTTPTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeCriblHTTP
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    auth_tokens: NotRequired[List[ItemsTypeAuthTokensTypedDict]]
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments."""
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputCriblHTTP(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeCriblHTTP

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    auth_tokens: Annotated[
        Optional[List[ItemsTypeAuthTokens]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments."""

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "authTokens",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeCriblTCP(str, Enum):
    CRIBL_TCP = "cribl_tcp"


class CreateInputInputCriblTCPTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeCriblTCP
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    enable_load_balancing: NotRequired[bool]
    r"""Load balance traffic across all Worker Processes"""
    auth_tokens: NotRequired[List[ItemsTypeAuthTokensTypedDict]]
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments."""
    description: NotRequired[str]


class CreateInputInputCriblTCP(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeCriblTCP

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        None
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = None
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = None
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    enable_load_balancing: Annotated[
        Optional[bool], pydantic.Field(alias="enableLoadBalancing")
    ] = None
    r"""Load balance traffic across all Worker Processes"""

    auth_tokens: Annotated[
        Optional[List[ItemsTypeAuthTokens]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments."""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "maxActiveCxn",
                "socketIdleTimeout",
                "socketEndingMaxWait",
                "socketMaxLifespan",
                "enableProxyHeader",
                "metadata",
                "enableLoadBalancing",
                "authTokens",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeCribl(str, Enum):
    CRIBL = "cribl"


class CreateInputInputCriblTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeCribl
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    filter_: NotRequired[str]
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputCribl(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeCribl

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    filter_: Annotated[Optional[str], pydantic.Field(alias="filter")] = None

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "filter",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeGooglePubsub(str, Enum):
    GOOGLE_PUBSUB = "google_pubsub"


class CreateInputInputGooglePubsubTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeGooglePubsub
    topic_name: str
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[GoogleAuthenticationMethodOptions]
    r"""Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class CreateInputInputGooglePubsub(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeGooglePubsub

    topic_name: Annotated[str, pydantic.Field(alias="topicName")]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = None
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = None
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = None
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Optional[GoogleAuthenticationMethodOptions],
        pydantic.Field(alias="googleAuthMethod"),
    ] = None
    r"""Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = None
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = None
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = None
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.GoogleAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "monitorSubscription",
                "createTopic",
                "createSubscription",
                "region",
                "googleAuthMethod",
                "serviceAccountCredentials",
                "secret",
                "maxBacklog",
                "concurrency",
                "requestTimeout",
                "metadata",
                "description",
                "orderedDelivery",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeFirehose(str, Enum):
    FIREHOSE = "firehose"


class CreateInputInputFirehoseTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeFirehose
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputFirehose(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeFirehose

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "authTokens",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputExecType(str, Enum):
    EXEC = "exec"


class CreateInputScheduleType(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class CreateInputInputExecTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputInputExecType
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    schedule_type: NotRequired[CreateInputScheduleType]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class CreateInputInputExec(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputInputExecType

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    retries: Optional[float] = None
    r"""Maximum number of retry attempts in the event that the command fails"""

    schedule_type: Annotated[
        Optional[CreateInputScheduleType], pydantic.Field(alias="scheduleType")
    ] = None
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = None
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = None
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.CreateInputScheduleType(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "retries",
                "scheduleType",
                "breakerRulesets",
                "staleChannelFlushMs",
                "metadata",
                "description",
                "interval",
                "cronSchedule",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeEventhub(str, Enum):
    EVENTHUB = "eventhub"


class CreateInputInputEventhubTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeEventhub
    brokers: List[str]
    r"""List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies."""
    topics: List[str]
    r"""The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group this instance belongs to. Default is 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Start reading from earliest available data; relevant only during initial subscription"""
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    sasl: NotRequired[AuthenticationType1TypedDict]
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""
    tls: NotRequired[TLSSettingsClientSideTypeTypedDict]
    session_timeout: NotRequired[float]
    r"""Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
    Value must be lower than rebalanceTimeout.
    See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    minimize_duplicates: NotRequired[bool]
    r"""Minimize duplicate events by starting only one consumer for each topic partition"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputEventhub(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeEventhub

    brokers: List[str]
    r"""List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies."""

    topics: List[str]
    r"""The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = None
    r"""The consumer group this instance belongs to. Default is 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        None
    )
    r"""Start reading from earliest available data; relevant only during initial subscription"""

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = None
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = None
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = None
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = None
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = None
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = None
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    sasl: Optional[AuthenticationType1] = None
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    tls: Optional[TLSSettingsClientSideType] = None

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = None
    r"""Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
    Value must be lower than rebalanceTimeout.
    See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = None
    r"""Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = None
    r"""Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
    """

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = None
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = None
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = None
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    minimize_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="minimizeDuplicates")
    ] = None
    r"""Minimize duplicate events by starting only one consumer for each topic partition"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "groupId",
                "fromBeginning",
                "connectionTimeout",
                "requestTimeout",
                "maxRetries",
                "maxBackOff",
                "initialBackoff",
                "backoffRate",
                "authenticationTimeout",
                "reauthenticationThreshold",
                "sasl",
                "tls",
                "sessionTimeout",
                "rebalanceTimeout",
                "heartbeatInterval",
                "autoCommitInterval",
                "autoCommitThreshold",
                "maxBytesPerPartition",
                "maxBytes",
                "maxSocketErrors",
                "minimizeDuplicates",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeOffice365MsgTrace(str, Enum):
    OFFICE365_MSG_TRACE = "office365_msg_trace"


class AuthenticationMethodOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select authentication method."""

    MANUAL = "manual"
    SECRET = "secret"
    OAUTH = "oauth"
    OAUTH_SECRET = "oauthSecret"
    OAUTH_CERT = "oauthCert"


class LogLevelOffice365MsgTrace(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Log Level (verbosity) for collection runtime behavior."""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"
    SILLY = "silly"


class CreateInputCertOptionsTypedDict(TypedDict):
    priv_key_path: str
    r"""Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS."""
    cert_path: str
    r"""Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS."""
    certificate_name: NotRequired[str]
    r"""The name of the predefined certificate."""
    passphrase: NotRequired[str]
    r"""Passphrase to use to decrypt the private key."""


class CreateInputCertOptions(BaseModel):
    priv_key_path: Annotated[str, pydantic.Field(alias="privKeyPath")]
    r"""Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS."""

    cert_path: Annotated[str, pydantic.Field(alias="certPath")]
    r"""Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS."""

    certificate_name: Annotated[
        Optional[str], pydantic.Field(alias="certificateName")
    ] = None
    r"""The name of the predefined certificate."""

    passphrase: Optional[str] = None
    r"""Passphrase to use to decrypt the private key."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["certificateName", "passphrase"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputOffice365MsgTraceTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeOffice365MsgTrace
    url: str
    r"""URL to use when retrieving report data."""
    interval: float
    r"""How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    start_date: NotRequired[str]
    r"""Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps."""
    end_date: NotRequired[str]
    r"""Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps."""
    timeout: NotRequired[float]
    r"""HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely."""
    disable_time_filter: NotRequired[bool]
    r"""Disables time filtering of events when a date range is specified."""
    auth_type: NotRequired[AuthenticationMethodOffice365MsgTrace]
    r"""Select authentication method."""
    reschedule_dropped_tasks: NotRequired[bool]
    r"""Reschedule tasks that failed with non-fatal errors"""
    max_task_reschedule: NotRequired[float]
    r"""Maximum number of times a task can be rescheduled"""
    log_level: NotRequired[LogLevelOffice365MsgTrace]
    r"""Log Level (verbosity) for collection runtime behavior."""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    retry_rules: NotRequired[RetryRulesType1TypedDict]
    description: NotRequired[str]
    username: NotRequired[str]
    r"""Username to run Message Trace API call."""
    password: NotRequired[str]
    r"""Password to run Message Trace API call."""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials."""
    client_secret: NotRequired[str]
    r"""client_secret to pass in the OAuth request parameter."""
    tenant_id: NotRequired[str]
    r"""Directory ID (tenant identifier) in Azure Active Directory."""
    client_id: NotRequired[str]
    r"""client_id to pass in the OAuth request parameter."""
    resource: NotRequired[str]
    r"""Resource to pass in the OAuth request parameter."""
    plan_type: NotRequired[SubscriptionPlanOptions]
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""
    text_secret: NotRequired[str]
    r"""Select or create a secret that references your client_secret to pass in the OAuth request parameter."""
    cert_options: NotRequired[CreateInputCertOptionsTypedDict]


class CreateInputInputOffice365MsgTrace(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeOffice365MsgTrace

    url: str
    r"""URL to use when retrieving report data."""

    interval: float
    r"""How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    start_date: Annotated[Optional[str], pydantic.Field(alias="startDate")] = None
    r"""Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps."""

    end_date: Annotated[Optional[str], pydantic.Field(alias="endDate")] = None
    r"""Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps."""

    timeout: Optional[float] = None
    r"""HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = None
    r"""Disables time filtering of events when a date range is specified."""

    auth_type: Annotated[
        Optional[AuthenticationMethodOffice365MsgTrace],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Select authentication method."""

    reschedule_dropped_tasks: Annotated[
        Optional[bool], pydantic.Field(alias="rescheduleDroppedTasks")
    ] = None
    r"""Reschedule tasks that failed with non-fatal errors"""

    max_task_reschedule: Annotated[
        Optional[float], pydantic.Field(alias="maxTaskReschedule")
    ] = None
    r"""Maximum number of times a task can be rescheduled"""

    log_level: Annotated[
        Optional[LogLevelOffice365MsgTrace], pydantic.Field(alias="logLevel")
    ] = None
    r"""Log Level (verbosity) for collection runtime behavior."""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = None
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = None
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    retry_rules: Annotated[
        Optional[RetryRulesType1], pydantic.Field(alias="retryRules")
    ] = None

    description: Optional[str] = None

    username: Optional[str] = None
    r"""Username to run Message Trace API call."""

    password: Optional[str] = None
    r"""Password to run Message Trace API call."""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials."""

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""client_secret to pass in the OAuth request parameter."""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""Directory ID (tenant identifier) in Azure Active Directory."""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""client_id to pass in the OAuth request parameter."""

    resource: Optional[str] = None
    r"""Resource to pass in the OAuth request parameter."""

    plan_type: Annotated[
        Optional[SubscriptionPlanOptions], pydantic.Field(alias="planType")
    ] = None
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a secret that references your client_secret to pass in the OAuth request parameter."""

    cert_options: Annotated[
        Optional[CreateInputCertOptions], pydantic.Field(alias="certOptions")
    ] = None

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOffice365MsgTrace(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOffice365MsgTrace(value)
            except ValueError:
                return value
        return value

    @field_serializer("plan_type")
    def serialize_plan_type(self, value):
        if isinstance(value, str):
            try:
                return models.SubscriptionPlanOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "startDate",
                "endDate",
                "timeout",
                "disableTimeFilter",
                "authType",
                "rescheduleDroppedTasks",
                "maxTaskReschedule",
                "logLevel",
                "jobTimeout",
                "keepAliveTime",
                "maxMissedKeepAlives",
                "ttl",
                "ignoreGroupJobsLimit",
                "metadata",
                "retryRules",
                "description",
                "username",
                "password",
                "credentialsSecret",
                "clientSecret",
                "tenantId",
                "clientId",
                "resource",
                "planType",
                "textSecret",
                "certOptions",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeOffice365Service(str, Enum):
    OFFICE365_SERVICE = "office365_service"


class ContentConfigOffice365ServiceTypedDict(TypedDict):
    content_type: NotRequired[str]
    r"""Office 365 Services API Content Type"""
    description: NotRequired[str]
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""
    interval: NotRequired[float]
    log_level: NotRequired[LogLevelOptionsContentConfigItems]
    r"""Collector runtime Log Level"""
    enabled: NotRequired[bool]


class ContentConfigOffice365Service(BaseModel):
    content_type: Annotated[Optional[str], pydantic.Field(alias="contentType")] = None
    r"""Office 365 Services API Content Type"""

    description: Optional[str] = None
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""

    interval: Optional[float] = None

    log_level: Annotated[
        Optional[LogLevelOptionsContentConfigItems], pydantic.Field(alias="logLevel")
    ] = None
    r"""Collector runtime Log Level"""

    enabled: Optional[bool] = None

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptionsContentConfigItems(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["contentType", "description", "interval", "logLevel", "enabled"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputOffice365ServiceTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeOffice365Service
    tenant_id: str
    r"""Office 365 Azure Tenant ID"""
    app_id: str
    r"""Office 365 Azure Application ID"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    plan_type: NotRequired[SubscriptionPlanOptions]
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""
    timeout: NotRequired[float]
    r"""HTTP request inactivity timeout, use 0 to disable"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    content_config: NotRequired[List[ContentConfigOffice365ServiceTypedDict]]
    r"""Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule."""
    retry_rules: NotRequired[RetryRulesType1TypedDict]
    auth_type: NotRequired[AuthenticationMethodOptions1]
    r"""Enter client secret directly, or select a stored secret"""
    description: NotRequired[str]
    client_secret: NotRequired[str]
    r"""Office 365 Azure client secret"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class CreateInputInputOffice365Service(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeOffice365Service

    tenant_id: Annotated[str, pydantic.Field(alias="tenantId")]
    r"""Office 365 Azure Tenant ID"""

    app_id: Annotated[str, pydantic.Field(alias="appId")]
    r"""Office 365 Azure Application ID"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    plan_type: Annotated[
        Optional[SubscriptionPlanOptions], pydantic.Field(alias="planType")
    ] = None
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    timeout: Optional[float] = None
    r"""HTTP request inactivity timeout, use 0 to disable"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = None
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = None
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    content_config: Annotated[
        Optional[List[ContentConfigOffice365Service]],
        pydantic.Field(alias="contentConfig"),
    ] = None
    r"""Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule."""

    retry_rules: Annotated[
        Optional[RetryRulesType1], pydantic.Field(alias="retryRules")
    ] = None

    auth_type: Annotated[
        Optional[AuthenticationMethodOptions1], pydantic.Field(alias="authType")
    ] = None
    r"""Enter client secret directly, or select a stored secret"""

    description: Optional[str] = None

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""Office 365 Azure client secret"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @field_serializer("plan_type")
    def serialize_plan_type(self, value):
        if isinstance(value, str):
            try:
                return models.SubscriptionPlanOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions1(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "planType",
                "timeout",
                "keepAliveTime",
                "jobTimeout",
                "maxMissedKeepAlives",
                "ttl",
                "ignoreGroupJobsLimit",
                "metadata",
                "contentConfig",
                "retryRules",
                "authType",
                "description",
                "clientSecret",
                "textSecret",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeOffice365Mgmt(str, Enum):
    OFFICE365_MGMT = "office365_mgmt"


class ContentConfigOffice365MgmtTypedDict(TypedDict):
    content_type: NotRequired[str]
    r"""Office 365 Management Activity API Content Type"""
    description: NotRequired[str]
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""
    interval: NotRequired[float]
    log_level: NotRequired[LogLevelOptionsContentConfigItems]
    r"""Collector runtime Log Level"""
    enabled: NotRequired[bool]


class ContentConfigOffice365Mgmt(BaseModel):
    content_type: Annotated[Optional[str], pydantic.Field(alias="contentType")] = None
    r"""Office 365 Management Activity API Content Type"""

    description: Optional[str] = None
    r"""If interval type is minutes the value entered must evenly divisible by 60 or save will fail"""

    interval: Optional[float] = None

    log_level: Annotated[
        Optional[LogLevelOptionsContentConfigItems], pydantic.Field(alias="logLevel")
    ] = None
    r"""Collector runtime Log Level"""

    enabled: Optional[bool] = None

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptionsContentConfigItems(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["contentType", "description", "interval", "logLevel", "enabled"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputOffice365MgmtTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeOffice365Mgmt
    plan_type: SubscriptionPlanOptions
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""
    tenant_id: str
    r"""Office 365 Azure Tenant ID"""
    app_id: str
    r"""Office 365 Azure Application ID"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    timeout: NotRequired[float]
    r"""HTTP request inactivity timeout, use 0 to disable"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    publisher_identifier: NotRequired[str]
    r"""Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)"""
    content_config: NotRequired[List[ContentConfigOffice365MgmtTypedDict]]
    r"""Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule."""
    ingestion_lag: NotRequired[float]
    r"""Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval."""
    retry_rules: NotRequired[RetryRulesType1TypedDict]
    auth_type: NotRequired[AuthenticationMethodOptions1]
    r"""Enter client secret directly, or select a stored secret"""
    description: NotRequired[str]
    client_secret: NotRequired[str]
    r"""Office 365 Azure client secret"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class CreateInputInputOffice365Mgmt(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeOffice365Mgmt

    plan_type: Annotated[SubscriptionPlanOptions, pydantic.Field(alias="planType")]
    r"""Office 365 subscription plan for your organization, typically Office 365 Enterprise"""

    tenant_id: Annotated[str, pydantic.Field(alias="tenantId")]
    r"""Office 365 Azure Tenant ID"""

    app_id: Annotated[str, pydantic.Field(alias="appId")]
    r"""Office 365 Azure Application ID"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    timeout: Optional[float] = None
    r"""HTTP request inactivity timeout, use 0 to disable"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = None
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = None
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    publisher_identifier: Annotated[
        Optional[str], pydantic.Field(alias="publisherIdentifier")
    ] = None
    r"""Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)"""

    content_config: Annotated[
        Optional[List[ContentConfigOffice365Mgmt]],
        pydantic.Field(alias="contentConfig"),
    ] = None
    r"""Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule."""

    ingestion_lag: Annotated[Optional[float], pydantic.Field(alias="ingestionLag")] = (
        None
    )
    r"""Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval."""

    retry_rules: Annotated[
        Optional[RetryRulesType1], pydantic.Field(alias="retryRules")
    ] = None

    auth_type: Annotated[
        Optional[AuthenticationMethodOptions1], pydantic.Field(alias="authType")
    ] = None
    r"""Enter client secret directly, or select a stored secret"""

    description: Optional[str] = None

    client_secret: Annotated[Optional[str], pydantic.Field(alias="clientSecret")] = None
    r"""Office 365 Azure client secret"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @field_serializer("plan_type")
    def serialize_plan_type(self, value):
        if isinstance(value, str):
            try:
                return models.SubscriptionPlanOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions1(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "timeout",
                "keepAliveTime",
                "jobTimeout",
                "maxMissedKeepAlives",
                "ttl",
                "ignoreGroupJobsLimit",
                "metadata",
                "publisherIdentifier",
                "contentConfig",
                "ingestionLag",
                "retryRules",
                "authType",
                "description",
                "clientSecret",
                "textSecret",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeEdgePrometheus(str, Enum):
    EDGE_PROMETHEUS = "edge_prometheus"


class DiscoveryTypeEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"
    # Kubernetes Node
    K8S_NODE = "k8s-node"
    # Kubernetes Pods
    K8S_PODS = "k8s-pods"


class AuthenticationMethodEdgePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter credentials directly, or select a stored secret"""

    MANUAL = "manual"
    SECRET = "secret"
    KUBERNETES = "kubernetes"


class CreateInputTargetTypedDict(TypedDict):
    host: str
    r"""Name of host from which to pull metrics."""
    protocol: NotRequired[ProtocolOptionsTargetsItems]
    r"""Protocol to use when collecting metrics"""
    port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""


class CreateInputTarget(BaseModel):
    host: str
    r"""Name of host from which to pull metrics."""

    protocol: Optional[ProtocolOptionsTargetsItems] = None
    r"""Protocol to use when collecting metrics"""

    port: Optional[float] = None
    r"""The port number in the metrics URL for discovered targets."""

    path: Optional[str] = None
    r"""Path to use when collecting metrics from discovered targets"""

    @field_serializer("protocol")
    def serialize_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ProtocolOptionsTargetsItems(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["protocol", "port", "path"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputPodFilterTypedDict(TypedDict):
    filter_: str
    r"""JavaScript expression applied to pods objects. Return 'true' to include it."""
    description: NotRequired[str]
    r"""Optional description of this rule's purpose"""


class CreateInputPodFilter(BaseModel):
    filter_: Annotated[str, pydantic.Field(alias="filter")]
    r"""JavaScript expression applied to pods objects. Return 'true' to include it."""

    description: Optional[str] = None
    r"""Optional description of this rule's purpose"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputEdgePrometheusTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeEdgePrometheus
    discovery_type: DiscoveryTypeEdgePrometheus
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: float
    r"""How often in seconds to scrape targets for metrics."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    timeout: NotRequired[float]
    r"""Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable"""
    persistence: NotRequired[DiskSpoolingTypeTypedDict]
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthenticationMethodEdgePrometheus]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    targets: NotRequired[List[CreateInputTargetTypedDict]]
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ProtocolOptionsTargetsItems]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Disable to use the private IP address."""
    search_filter: NotRequired[List[ItemsTypeSearchFilterTypedDict]]
    r"""Filter to apply when searching for EC2 instances"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions1]
    r"""Signature version to use for signing EC2 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    scrape_protocol_expr: NotRequired[str]
    r"""Protocol to use when collecting metrics"""
    scrape_port_expr: NotRequired[str]
    r"""The port number in the metrics URL for discovered targets."""
    scrape_path_expr: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    pod_filter: NotRequired[List[CreateInputPodFilterTypedDict]]
    r"""Add rules to decide which pods to discover for metrics.
    Pods are searched if no rules are given or of all the rules'
    expressions evaluate to true.

    """
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class CreateInputInputEdgePrometheus(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeEdgePrometheus

    discovery_type: Annotated[
        DiscoveryTypeEdgePrometheus, pydantic.Field(alias="discoveryType")
    ]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: float
    r"""How often in seconds to scrape targets for metrics."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    timeout: Optional[float] = None
    r"""Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable"""

    persistence: Optional[DiskSpoolingType] = None

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Optional[AuthenticationMethodEdgePrometheus], pydantic.Field(alias="authType")
    ] = None
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    targets: Optional[List[CreateInputTarget]] = None

    record_type: Annotated[
        Optional[RecordTypeOptions], pydantic.Field(alias="recordType")
    ] = None
    r"""DNS record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = None
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Optional[ProtocolOptionsTargetsItems], pydantic.Field(alias="scrapeProtocol")
    ] = None
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = None
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = None
    r"""Use public IP address for discovered targets. Disable to use the private IP address."""

    search_filter: Annotated[
        Optional[List[ItemsTypeSearchFilter]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""Filter to apply when searching for EC2 instances"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptions1], pydantic.Field(alias="signatureVersion")
    ] = None
    r"""Signature version to use for signing EC2 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    scrape_protocol_expr: Annotated[
        Optional[str], pydantic.Field(alias="scrapeProtocolExpr")
    ] = None
    r"""Protocol to use when collecting metrics"""

    scrape_port_expr: Annotated[
        Optional[str], pydantic.Field(alias="scrapePortExpr")
    ] = None
    r"""The port number in the metrics URL for discovered targets."""

    scrape_path_expr: Annotated[
        Optional[str], pydantic.Field(alias="scrapePathExpr")
    ] = None
    r"""Path to use when collecting metrics from discovered targets"""

    pod_filter: Annotated[
        Optional[List[CreateInputPodFilter]], pydantic.Field(alias="podFilter")
    ] = None
    r"""Add rules to decide which pods to discover for metrics.
    Pods are searched if no rules are given or of all the rules'
    expressions evaluate to true.

    """

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.DiscoveryTypeEdgePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodEdgePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ProtocolOptionsTargetsItems(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions1(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "dimensionList",
                "timeout",
                "persistence",
                "metadata",
                "authType",
                "description",
                "targets",
                "recordType",
                "scrapePort",
                "nameList",
                "scrapeProtocol",
                "scrapePath",
                "awsAuthenticationMethod",
                "awsApiKey",
                "awsSecret",
                "usePublicIp",
                "searchFilter",
                "awsSecretKey",
                "region",
                "endpoint",
                "signatureVersion",
                "reuseConnections",
                "rejectUnauthorized",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "scrapeProtocolExpr",
                "scrapePortExpr",
                "scrapePathExpr",
                "podFilter",
                "username",
                "password",
                "credentialsSecret",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypePrometheus(str, Enum):
    PROMETHEUS = "prometheus"


class DiscoveryTypePrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class LogLevelPrometheus(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime log level"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class CreateInputMetricsProtocol(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Protocol to use when collecting metrics"""

    HTTP = "http"
    HTTPS = "https"


class CreateInputInputPrometheusTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypePrometheus
    interval: float
    r"""How often, in minutes, to scrape targets for metrics. Maximum of 60 minutes. 60 must be evenly divisible by the value you enter."""
    log_level: LogLevelPrometheus
    r"""Collector runtime log level"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[DiscoveryTypePrometheus]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    timeout: NotRequired[float]
    r"""Time, in seconds, before aborting HTTP connection attempts; use 0 for no timeout"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthenticationMethodOptionsSasl]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets"""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[CreateInputMetricsProtocol]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Disable to use the private IP address."""
    search_filter: NotRequired[List[ItemsTypeSearchFilterTypedDict]]
    r"""Filter to apply when searching for EC2 instances"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions1]
    r"""Signature version to use for signing EC2 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class CreateInputInputPrometheus(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypePrometheus

    interval: float
    r"""How often, in minutes, to scrape targets for metrics. Maximum of 60 minutes. 60 must be evenly divisible by the value you enter."""

    log_level: Annotated[LogLevelPrometheus, pydantic.Field(alias="logLevel")]
    r"""Collector runtime log level"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Optional[DiscoveryTypePrometheus], pydantic.Field(alias="discoveryType")
    ] = None
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    timeout: Optional[float] = None
    r"""Time, in seconds, before aborting HTTP connection attempts; use 0 for no timeout"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = None
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = None
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Optional[AuthenticationMethodOptionsSasl], pydantic.Field(alias="authType")
    ] = None
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Optional[RecordTypeOptions], pydantic.Field(alias="recordType")
    ] = None
    r"""DNS record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = None
    r"""The port number in the metrics URL for discovered targets"""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Optional[CreateInputMetricsProtocol], pydantic.Field(alias="scrapeProtocol")
    ] = None
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = None
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = None
    r"""Use public IP address for discovered targets. Disable to use the private IP address."""

    search_filter: Annotated[
        Optional[List[ItemsTypeSearchFilter]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""Filter to apply when searching for EC2 instances"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptions1], pydantic.Field(alias="signatureVersion")
    ] = None
    r"""Signature version to use for signing EC2 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.DiscoveryTypePrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelPrometheus(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsSasl(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.CreateInputMetricsProtocol(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions1(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "dimensionList",
                "discoveryType",
                "rejectUnauthorized",
                "timeout",
                "keepAliveTime",
                "jobTimeout",
                "maxMissedKeepAlives",
                "ttl",
                "ignoreGroupJobsLimit",
                "metadata",
                "authType",
                "description",
                "targetList",
                "recordType",
                "scrapePort",
                "nameList",
                "scrapeProtocol",
                "scrapePath",
                "awsAuthenticationMethod",
                "awsApiKey",
                "awsSecret",
                "usePublicIp",
                "searchFilter",
                "awsSecretKey",
                "region",
                "endpoint",
                "signatureVersion",
                "reuseConnections",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "username",
                "password",
                "credentialsSecret",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypePrometheusRw(str, Enum):
    PROMETHEUS_RW = "prometheus_rw"


class CreateInputInputPrometheusRwTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypePrometheusRw
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    prometheus_api: str
    r"""Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<yourupstreamURL>:<yourport>/write."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    auth_type: NotRequired[AuthenticationTypeOptionsPrometheusAuth]
    r"""Remote Write authentication type"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputInputPrometheusRw(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypePrometheusRw

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    prometheus_api: Annotated[str, pydantic.Field(alias="prometheusAPI")]
    r"""Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<yourupstreamURL>:<yourport>/write."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    auth_type: Annotated[
        Optional[AuthenticationTypeOptionsPrometheusAuth],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Remote Write authentication type"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeOptionsPrometheusAuth(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "authType",
                "metadata",
                "description",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeLoki(str, Enum):
    LOKI = "loki"


class CreateInputInputLokiTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeLoki
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    loki_api: str
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    auth_type: NotRequired[AuthenticationTypeOptionsLokiAuth]
    r"""Loki logs authentication type"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputInputLoki(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeLoki

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    loki_api: Annotated[str, pydantic.Field(alias="lokiAPI")]
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    auth_type: Annotated[
        Optional[AuthenticationTypeOptionsLokiAuth], pydantic.Field(alias="authType")
    ] = None
    r"""Loki logs authentication type"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeOptionsLokiAuth(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "authType",
                "metadata",
                "description",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputGrafanaType2(str, Enum):
    GRAFANA = "grafana"


class CreateInputPrometheusAuth2TypedDict(TypedDict):
    auth_type: NotRequired[AuthenticationTypeOptionsPrometheusAuth]
    r"""Remote Write authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputPrometheusAuth2(BaseModel):
    auth_type: Annotated[
        Optional[AuthenticationTypeOptionsPrometheusAuth],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Remote Write authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeOptionsPrometheusAuth(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputLokiAuth2TypedDict(TypedDict):
    auth_type: NotRequired[AuthenticationTypeOptionsLokiAuth]
    r"""Loki logs authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputLokiAuth2(BaseModel):
    auth_type: Annotated[
        Optional[AuthenticationTypeOptionsLokiAuth], pydantic.Field(alias="authType")
    ] = None
    r"""Loki logs authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeOptionsLokiAuth(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputGrafanaGrafana2TypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputInputGrafanaType2
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    loki_api: str
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    prometheus_api: NotRequired[str]
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<yourupstreamURL>:<yourport>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""
    prometheus_auth: NotRequired[CreateInputPrometheusAuth2TypedDict]
    loki_auth: NotRequired[CreateInputLokiAuth2TypedDict]
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputGrafanaGrafana2(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputInputGrafanaType2

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    loki_api: Annotated[str, pydantic.Field(alias="lokiAPI")]
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    prometheus_api: Annotated[Optional[str], pydantic.Field(alias="prometheusAPI")] = (
        None
    )
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<yourupstreamURL>:<yourport>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""

    prometheus_auth: Annotated[
        Optional[CreateInputPrometheusAuth2], pydantic.Field(alias="prometheusAuth")
    ] = None

    loki_auth: Annotated[
        Optional[CreateInputLokiAuth2], pydantic.Field(alias="lokiAuth")
    ] = None

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "prometheusAPI",
                "prometheusAuth",
                "lokiAuth",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputGrafanaType1(str, Enum):
    GRAFANA = "grafana"


class CreateInputPrometheusAuth1TypedDict(TypedDict):
    auth_type: NotRequired[AuthenticationTypeOptionsPrometheusAuth]
    r"""Remote Write authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputPrometheusAuth1(BaseModel):
    auth_type: Annotated[
        Optional[AuthenticationTypeOptionsPrometheusAuth],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Remote Write authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeOptionsPrometheusAuth(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputLokiAuth1TypedDict(TypedDict):
    auth_type: NotRequired[AuthenticationTypeOptionsLokiAuth]
    r"""Loki logs authentication type"""
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputLokiAuth1(BaseModel):
    auth_type: Annotated[
        Optional[AuthenticationTypeOptionsLokiAuth], pydantic.Field(alias="authType")
    ] = None
    r"""Loki logs authentication type"""

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeOptionsLokiAuth(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputGrafanaGrafana1TypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputInputGrafanaType1
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    prometheus_api: str
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<yourupstreamURL>:<yourport>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    loki_api: NotRequired[str]
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""
    prometheus_auth: NotRequired[CreateInputPrometheusAuth1TypedDict]
    loki_auth: NotRequired[CreateInputLokiAuth1TypedDict]
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputGrafanaGrafana1(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputInputGrafanaType1

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    prometheus_api: Annotated[str, pydantic.Field(alias="prometheusAPI")]
    r"""Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<yourupstreamURL>:<yourport>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    loki_api: Annotated[Optional[str], pydantic.Field(alias="lokiAPI")] = None
    r"""Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured."""

    prometheus_auth: Annotated[
        Optional[CreateInputPrometheusAuth1], pydantic.Field(alias="prometheusAuth")
    ] = None

    loki_auth: Annotated[
        Optional[CreateInputLokiAuth1], pydantic.Field(alias="lokiAuth")
    ] = None

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "lokiAPI",
                "prometheusAuth",
                "lokiAuth",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateInputInputGrafanaUnionTypedDict = TypeAliasType(
    "CreateInputInputGrafanaUnionTypedDict",
    Union[
        CreateInputInputGrafanaGrafana1TypedDict,
        CreateInputInputGrafanaGrafana2TypedDict,
    ],
)


CreateInputInputGrafanaUnion = TypeAliasType(
    "CreateInputInputGrafanaUnion",
    Union[CreateInputInputGrafanaGrafana1, CreateInputInputGrafanaGrafana2],
)


class CreateInputTypeConfluentCloud(str, Enum):
    CONFLUENT_CLOUD = "confluent_cloud"


class CreateInputInputConfluentCloudTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeConfluentCloud
    brokers: List[str]
    r"""List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092"""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsClientSideTypeKafkaSchemaRegistryTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    kafka_schema_registry: NotRequired[KafkaSchemaRegistryAuthenticationTypeTypedDict]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    sasl: NotRequired[AuthenticationTypeTypedDict]
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""
    session_timeout: NotRequired[float]
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputConfluentCloud(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeConfluentCloud

    brokers: List[str]
    r"""List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092"""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsClientSideTypeKafkaSchemaRegistry] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = None
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        None
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    kafka_schema_registry: Annotated[
        Optional[KafkaSchemaRegistryAuthenticationType],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = None
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = None
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = None
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = None
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = None
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = None
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    sasl: Optional[AuthenticationType] = None
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = None
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = None
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = None
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = None
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = None
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = None
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "groupId",
                "fromBeginning",
                "kafkaSchemaRegistry",
                "connectionTimeout",
                "requestTimeout",
                "maxRetries",
                "maxBackOff",
                "initialBackoff",
                "backoffRate",
                "authenticationTimeout",
                "reauthenticationThreshold",
                "sasl",
                "sessionTimeout",
                "rebalanceTimeout",
                "heartbeatInterval",
                "autoCommitInterval",
                "autoCommitThreshold",
                "maxBytesPerPartition",
                "maxBytes",
                "maxSocketErrors",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeElastic(str, Enum):
    ELASTIC = "elastic"


class AuthenticationTypeElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    # None
    NONE = "none"
    # Basic
    BASIC = "basic"
    # Basic (credentials secret)
    CREDENTIALS_SECRET = "credentialsSecret"
    # Auth Tokens
    AUTH_TOKENS = "authTokens"


class CreateInputAPIVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The API version to use for communicating with the server"""

    # 6.8.4
    SIX_DOT_8_DOT_4 = "6.8.4"
    # 8.3.2
    EIGHT_DOT_3_DOT_2 = "8.3.2"
    # Custom
    CUSTOM = "custom"


class AuthenticationMethodElastic(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Enter credentials directly, or select a stored secret"""

    NONE = "none"
    MANUAL = "manual"
    SECRET = "secret"


class ProxyModeElasticTypedDict(TypedDict):
    enabled: bool
    r"""Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details."""
    auth_type: NotRequired[AuthenticationMethodElastic]
    r"""Enter credentials directly, or select a stored secret"""
    username: NotRequired[str]
    password: NotRequired[str]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    url: NotRequired[str]
    r"""URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""
    remove_headers: NotRequired[List[str]]
    r"""List of headers to remove from the request to proxy"""
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a proxy request to complete before canceling it"""


class ProxyModeElastic(BaseModel):
    enabled: bool
    r"""Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details."""

    auth_type: Annotated[
        Optional[AuthenticationMethodElastic], pydantic.Field(alias="authType")
    ] = None
    r"""Enter credentials directly, or select a stored secret"""

    username: Optional[str] = None

    password: Optional[str] = None

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    url: Optional[str] = None
    r"""URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""

    remove_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="removeHeaders")
    ] = None
    r"""List of headers to remove from the request to proxy"""

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = None
    r"""Amount of time, in seconds, to wait for a proxy request to complete before canceling it"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodElastic(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "username",
                "password",
                "credentialsSecret",
                "url",
                "rejectUnauthorized",
                "removeHeaders",
                "timeoutSec",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputElasticTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeElastic
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    elastic_api: str
    r"""Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    auth_type: NotRequired[AuthenticationTypeElastic]
    api_version: NotRequired[CreateInputAPIVersion]
    r"""The API version to use for communicating with the server"""
    extra_http_headers: NotRequired[List[ItemsTypeExtraHTTPHeadersTypedDict]]
    r"""Headers to add to all events"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    proxy_mode: NotRequired[ProxyModeElasticTypedDict]
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    auth_tokens: NotRequired[List[str]]
    r"""Bearer tokens to include in the authorization header"""
    custom_api_version: NotRequired[str]
    r"""Custom version information to respond to requests"""


class CreateInputInputElastic(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeElastic

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    elastic_api: Annotated[str, pydantic.Field(alias="elasticAPI")]
    r"""Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    auth_type: Annotated[
        Optional[AuthenticationTypeElastic], pydantic.Field(alias="authType")
    ] = None

    api_version: Annotated[
        Optional[CreateInputAPIVersion], pydantic.Field(alias="apiVersion")
    ] = None
    r"""The API version to use for communicating with the server"""

    extra_http_headers: Annotated[
        Optional[List[ItemsTypeExtraHTTPHeaders]],
        pydantic.Field(alias="extraHttpHeaders"),
    ] = None
    r"""Headers to add to all events"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    proxy_mode: Annotated[
        Optional[ProxyModeElastic], pydantic.Field(alias="proxyMode")
    ] = None

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Bearer tokens to include in the authorization header"""

    custom_api_version: Annotated[
        Optional[str], pydantic.Field(alias="customAPIVersion")
    ] = None
    r"""Custom version information to respond to requests"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeElastic(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.CreateInputAPIVersion(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "authType",
                "apiVersion",
                "extraHttpHeaders",
                "metadata",
                "proxyMode",
                "description",
                "username",
                "password",
                "credentialsSecret",
                "authTokens",
                "customAPIVersion",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeAzureBlob(str, Enum):
    AZURE_BLOB = "azure_blob"


class CreateInputInputAzureBlobTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeAzureBlob
    queue_name: str
    r"""The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    visibility_timeout: NotRequired[float]
    r"""The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    max_messages: NotRequired[float]
    r"""The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32."""
    service_period_secs: NotRequired[float]
    r"""The duration (in seconds) which pollers should be validated and restarted if exited"""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    auth_type: NotRequired[AuthenticationMethodOptions]
    description: NotRequired[str]
    connection_string: NotRequired[str]
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    storage_account_name: NotRequired[str]
    r"""The name of your Azure storage account"""
    tenant_id: NotRequired[str]
    r"""The service principal's tenant ID"""
    client_id: NotRequired[str]
    r"""The service principal's client ID"""
    azure_cloud: NotRequired[str]
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""
    endpoint_suffix: NotRequired[str]
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    certificate: NotRequired[CertificateTypeAzureBlobAuthTypeClientCertTypedDict]


class CreateInputInputAzureBlob(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeAzureBlob

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = None
    r"""Regex matching file names to download and process. Defaults to: .*"""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = None
    r"""The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = (
        None
    )
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = None
    r"""The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32."""

    service_period_secs: Annotated[
        Optional[float], pydantic.Field(alias="servicePeriodSecs")
    ] = None
    r"""The duration (in seconds) which pollers should be validated and restarted if exited"""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = None
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    auth_type: Annotated[
        Optional[AuthenticationMethodOptions], pydantic.Field(alias="authType")
    ] = None

    description: Optional[str] = None

    connection_string: Annotated[
        Optional[str], pydantic.Field(alias="connectionString")
    ] = None
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    storage_account_name: Annotated[
        Optional[str], pydantic.Field(alias="storageAccountName")
    ] = None
    r"""The name of your Azure storage account"""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""The service principal's tenant ID"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""The service principal's client ID"""

    azure_cloud: Annotated[Optional[str], pydantic.Field(alias="azureCloud")] = None
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""

    endpoint_suffix: Annotated[
        Optional[str], pydantic.Field(alias="endpointSuffix")
    ] = None
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    certificate: Optional[CertificateTypeAzureBlobAuthTypeClientCert] = None

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "fileFilter",
                "visibilityTimeout",
                "numReceivers",
                "maxMessages",
                "servicePeriodSecs",
                "skipOnError",
                "metadata",
                "breakerRulesets",
                "staleChannelFlushMs",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "authType",
                "description",
                "connectionString",
                "textSecret",
                "storageAccountName",
                "tenantId",
                "clientId",
                "azureCloud",
                "endpointSuffix",
                "clientTextSecret",
                "certificate",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeSplunkHec(str, Enum):
    SPLUNK_HEC = "splunk_hec"


class AuthTokenSplunkHecTypedDict(TypedDict):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""
    auth_type: NotRequired[AuthenticationMethodOptionsAuthTokensItems]
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""
    token_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    enabled: NotRequired[bool]
    description: NotRequired[str]
    r"""Optional token description"""
    allowed_indexes_at_token: NotRequired[List[str]]
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events referencing this token"""


class AuthTokenSplunkHec(BaseModel):
    token: str
    r"""Shared secret to be provided by any client (Authorization: <token>)"""

    auth_type: Annotated[
        Optional[AuthenticationMethodOptionsAuthTokensItems],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate"""

    token_secret: Annotated[Optional[str], pydantic.Field(alias="tokenSecret")] = None
    r"""Select or create a stored text secret"""

    enabled: Optional[bool] = None

    description: Optional[str] = None
    r"""Optional token description"""

    allowed_indexes_at_token: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexesAtToken")
    ] = None
    r"""Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events referencing this token"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsAuthTokensItems(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "tokenSecret",
                "enabled",
                "description",
                "allowedIndexesAtToken",
                "metadata",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputInputSplunkHecTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeSplunkHec
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    splunk_hec_api: str
    r"""Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    auth_tokens: NotRequired[List[AuthTokenSplunkHecTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[Any]
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info."""
    allowed_indexes: NotRequired[List[str]]
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""
    splunk_hec_acks: NotRequired[bool]
    r"""Enable Splunk HEC acknowledgements"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    use_fwd_timezone: NotRequired[bool]
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""
    drop_control_fields: NotRequired[bool]
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""
    extract_metrics: NotRequired[bool]
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""
    access_control_allow_origin: NotRequired[List[str]]
    r"""Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""
    access_control_allow_headers: NotRequired[List[str]]
    r"""Optionally, list HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""
    emit_token_metrics: NotRequired[bool]
    r"""Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""
    description: NotRequired[str]


class CreateInputInputSplunkHec(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeSplunkHec

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    splunk_hec_api: Annotated[str, pydantic.Field(alias="splunkHecAPI")]
    r"""Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    auth_tokens: Annotated[
        Optional[List[AuthTokenSplunkHec]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[Any], pydantic.Field(alias="enableHealthCheck")
    ] = None

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info."""

    allowed_indexes: Annotated[
        Optional[List[str]], pydantic.Field(alias="allowedIndexes")
    ] = None
    r"""List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = None
    r"""Enable Splunk HEC acknowledgements"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    use_fwd_timezone: Annotated[
        Optional[bool], pydantic.Field(alias="useFwdTimezone")
    ] = None
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""

    drop_control_fields: Annotated[
        Optional[bool], pydantic.Field(alias="dropControlFields")
    ] = None
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""

    extract_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="extractMetrics")
    ] = None
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""

    access_control_allow_origin: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowOrigin")
    ] = None
    r"""Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards."""

    access_control_allow_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="accessControlAllowHeaders")
    ] = None
    r"""Optionally, list HTTP headers that @{product} will send to allowed origins as \"Access-Control-Allow-Headers\" in a CORS preflight response. Use \"*\" to allow all headers."""

    emit_token_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="emitTokenMetrics")
    ] = None
    r"""Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "authTokens",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "metadata",
                "allowedIndexes",
                "splunkHecAcks",
                "breakerRulesets",
                "staleChannelFlushMs",
                "useFwdTimezone",
                "dropControlFields",
                "extractMetrics",
                "accessControlAllowOrigin",
                "accessControlAllowHeaders",
                "emitTokenMetrics",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeSplunkSearch(str, Enum):
    SPLUNK_SEARCH = "splunk_search"


class CreateInputEndpointParamTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g.,`${earliest}`). Ifa constant, use single quotes (e.g.,'earliest'). Valueswithout delimiters (e.g.,earliest) are evaluated as strings."""


class CreateInputEndpointParam(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g.,`${earliest}`). Ifa constant, use single quotes (e.g.,'earliest'). Valueswithout delimiters (e.g.,earliest) are evaluated as strings."""


class CreateInputEndpointHeaderTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute the header's value, normally enclosed in backticks (e.g.,`${earliest}`). Ifa constant, use single quotes (e.g.,'earliest'). Valueswithout delimiters (e.g.,earliest) are evaluated as strings."""


class CreateInputEndpointHeader(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute the header's value, normally enclosed in backticks (e.g.,`${earliest}`). Ifa constant, use single quotes (e.g.,'earliest'). Valueswithout delimiters (e.g.,earliest) are evaluated as strings."""


class LogLevelSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime log level (verbosity)"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class AuthenticationTypeSplunkSearch(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Splunk Search authentication type"""

    NONE = "none"
    BASIC = "basic"
    CREDENTIALS_SECRET = "credentialsSecret"
    TOKEN = "token"
    TEXT_SECRET = "textSecret"
    OAUTH = "oauth"


class CreateInputInputSplunkSearchTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeSplunkSearch
    search_head: str
    r"""Search head base URL. Can be an expression. Default is https://localhost:8089."""
    search: str
    r"""Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'"""
    cron_schedule: str
    r"""A cron schedule on which to run this job"""
    endpoint: str
    r"""REST API used to create a search"""
    output_mode: OutputModeOptionsSplunkCollectorConf
    r"""Format of the returned output"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    earliest: NotRequired[str]
    r"""The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'"""
    latest: NotRequired[str]
    r"""The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'"""
    endpoint_params: NotRequired[List[CreateInputEndpointParamTypedDict]]
    r"""Optional request parameters to send to the endpoint"""
    endpoint_headers: NotRequired[List[CreateInputEndpointHeaderTypedDict]]
    r"""Optional request headers to send to the endpoint"""
    log_level: NotRequired[LogLevelSplunkSearch]
    r"""Collector runtime log level (verbosity)"""
    request_timeout: NotRequired[float]
    r"""HTTP request inactivity timeout. Use 0 for no timeout."""
    use_round_robin_dns: NotRequired[bool]
    r"""When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""
    encoding: NotRequired[str]
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    retry_rules: NotRequired[RetryRulesTypeTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    auth_type: NotRequired[AuthenticationTypeSplunkSearch]
    r"""Splunk Search authentication type"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    login_url: NotRequired[str]
    r"""URL for OAuth"""
    secret_param_name: NotRequired[str]
    r"""Secret parameter name to pass in request body"""
    secret: NotRequired[str]
    r"""Secret parameter value to pass in request body"""
    token_attribute_name: NotRequired[str]
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""
    auth_header_expr: NotRequired[str]
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""
    token_timeout_secs: NotRequired[float]
    r"""How often the OAuth token should be refreshed."""
    oauth_params: NotRequired[List[ItemsTypeOauthParamsTypedDict]]
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""
    oauth_headers: NotRequired[List[ItemsTypeOauthHeadersTypedDict]]
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""


class CreateInputInputSplunkSearch(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeSplunkSearch

    search_head: Annotated[str, pydantic.Field(alias="searchHead")]
    r"""Search head base URL. Can be an expression. Default is https://localhost:8089."""

    search: str
    r"""Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'"""

    cron_schedule: Annotated[str, pydantic.Field(alias="cronSchedule")]
    r"""A cron schedule on which to run this job"""

    endpoint: str
    r"""REST API used to create a search"""

    output_mode: Annotated[
        OutputModeOptionsSplunkCollectorConf, pydantic.Field(alias="outputMode")
    ]
    r"""Format of the returned output"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    earliest: Optional[str] = None
    r"""The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'"""

    latest: Optional[str] = None
    r"""The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'"""

    endpoint_params: Annotated[
        Optional[List[CreateInputEndpointParam]], pydantic.Field(alias="endpointParams")
    ] = None
    r"""Optional request parameters to send to the endpoint"""

    endpoint_headers: Annotated[
        Optional[List[CreateInputEndpointHeader]],
        pydantic.Field(alias="endpointHeaders"),
    ] = None
    r"""Optional request headers to send to the endpoint"""

    log_level: Annotated[
        Optional[LogLevelSplunkSearch], pydantic.Field(alias="logLevel")
    ] = None
    r"""Collector runtime log level (verbosity)"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""HTTP request inactivity timeout. Use 0 for no timeout."""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = None
    r"""When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""

    encoding: Optional[str] = None
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = None
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = None
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    retry_rules: Annotated[
        Optional[RetryRulesType], pydantic.Field(alias="retryRules")
    ] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    auth_type: Annotated[
        Optional[AuthenticationTypeSplunkSearch], pydantic.Field(alias="authType")
    ] = None
    r"""Splunk Search authentication type"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    login_url: Annotated[Optional[str], pydantic.Field(alias="loginUrl")] = None
    r"""URL for OAuth"""

    secret_param_name: Annotated[
        Optional[str], pydantic.Field(alias="secretParamName")
    ] = None
    r"""Secret parameter name to pass in request body"""

    secret: Optional[str] = None
    r"""Secret parameter value to pass in request body"""

    token_attribute_name: Annotated[
        Optional[str], pydantic.Field(alias="tokenAttributeName")
    ] = None
    r"""Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token')."""

    auth_header_expr: Annotated[
        Optional[str], pydantic.Field(alias="authHeaderExpr")
    ] = None
    r"""JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`."""

    token_timeout_secs: Annotated[
        Optional[float], pydantic.Field(alias="tokenTimeoutSecs")
    ] = None
    r"""How often the OAuth token should be refreshed."""

    oauth_params: Annotated[
        Optional[List[ItemsTypeOauthParams]], pydantic.Field(alias="oauthParams")
    ] = None
    r"""Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    oauth_headers: Annotated[
        Optional[List[ItemsTypeOauthHeaders]], pydantic.Field(alias="oauthHeaders")
    ] = None
    r"""Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request."""

    @field_serializer("output_mode")
    def serialize_output_mode(self, value):
        if isinstance(value, str):
            try:
                return models.OutputModeOptionsSplunkCollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelSplunkSearch(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationTypeSplunkSearch(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "earliest",
                "latest",
                "endpointParams",
                "endpointHeaders",
                "logLevel",
                "requestTimeout",
                "useRoundRobinDns",
                "rejectUnauthorized",
                "encoding",
                "keepAliveTime",
                "jobTimeout",
                "maxMissedKeepAlives",
                "ttl",
                "ignoreGroupJobsLimit",
                "metadata",
                "retryRules",
                "breakerRulesets",
                "staleChannelFlushMs",
                "authType",
                "description",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
                "loginUrl",
                "secretParamName",
                "secret",
                "tokenAttributeName",
                "authHeaderExpr",
                "tokenTimeoutSecs",
                "oauthParams",
                "oauthHeaders",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeSplunk(str, Enum):
    SPLUNK = "splunk"


class AuthTokenSplunkTypedDict(TypedDict):
    token: str
    r"""Shared secrets to be provided by any Splunk forwarder. Ifempty, unauthorized access is permitted."""
    description: NotRequired[str]


class AuthTokenSplunk(BaseModel):
    token: str
    r"""Shared secrets to be provided by any Splunk forwarder. Ifempty, unauthorized access is permitted."""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["description"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputMaxS2SVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The highest S2S protocol version to advertise during handshake"""

    # v3
    V3 = "v3"
    # v4
    V4 = "v4"


class CreateInputCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections."""

    # Disabled
    DISABLED = "disabled"
    # Automatic
    AUTO = "auto"
    # Always
    ALWAYS = "always"


class CreateInputInputSplunkTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeSplunk
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    ip_whitelist_regex: NotRequired[str]
    r"""Regex matching IP addresses that are allowed to establish a connection"""
    max_active_cxn: NotRequired[float]
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""
    socket_idle_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""
    socket_ending_max_wait: NotRequired[float]
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""
    socket_max_lifespan: NotRequired[float]
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""
    enable_proxy_header: NotRequired[bool]
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    auth_tokens: NotRequired[List[AuthTokenSplunkTypedDict]]
    r"""Shared secrets to be provided by any Splunk forwarder. Ifempty, unauthorized access is permitted."""
    max_s2_sversion: NotRequired[CreateInputMaxS2SVersion]
    r"""The highest S2S protocol version to advertise during handshake"""
    description: NotRequired[str]
    use_fwd_timezone: NotRequired[bool]
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""
    drop_control_fields: NotRequired[bool]
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""
    extract_metrics: NotRequired[bool]
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""
    compress: NotRequired[CreateInputCompression]
    r"""Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections."""


class CreateInputInputSplunk(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeSplunk

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    tls: Optional[TLSSettingsServerSideType] = None

    ip_whitelist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipWhitelistRegex")
    ] = None
    r"""Regex matching IP addresses that are allowed to establish a connection"""

    max_active_cxn: Annotated[Optional[float], pydantic.Field(alias="maxActiveCxn")] = (
        None
    )
    r"""Maximum number of active connections allowed per Worker Process. Use 0 for unlimited."""

    socket_idle_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketIdleTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring."""

    socket_ending_max_wait: Annotated[
        Optional[float], pydantic.Field(alias="socketEndingMaxWait")
    ] = None
    r"""How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring."""

    socket_max_lifespan: Annotated[
        Optional[float], pydantic.Field(alias="socketMaxLifespan")
    ] = None
    r"""The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Enable if the connection is proxied by a device that supports proxy protocol v1 or v2"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    auth_tokens: Annotated[
        Optional[List[AuthTokenSplunk]], pydantic.Field(alias="authTokens")
    ] = None
    r"""Shared secrets to be provided by any Splunk forwarder. Ifempty, unauthorized access is permitted."""

    max_s2_sversion: Annotated[
        Optional[CreateInputMaxS2SVersion], pydantic.Field(alias="maxS2Sversion")
    ] = None
    r"""The highest S2S protocol version to advertise during handshake"""

    description: Optional[str] = None

    use_fwd_timezone: Annotated[
        Optional[bool], pydantic.Field(alias="useFwdTimezone")
    ] = None
    r"""Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event"""

    drop_control_fields: Annotated[
        Optional[bool], pydantic.Field(alias="dropControlFields")
    ] = None
    r"""Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`."""

    extract_metrics: Annotated[
        Optional[bool], pydantic.Field(alias="extractMetrics")
    ] = None
    r"""Extract and process Splunk-generated metrics as Cribl metrics"""

    compress: Optional[CreateInputCompression] = None
    r"""Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections."""

    @field_serializer("max_s2_sversion")
    def serialize_max_s2_sversion(self, value):
        if isinstance(value, str):
            try:
                return models.CreateInputMaxS2SVersion(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CreateInputCompression(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "tls",
                "ipWhitelistRegex",
                "maxActiveCxn",
                "socketIdleTimeout",
                "socketEndingMaxWait",
                "socketMaxLifespan",
                "enableProxyHeader",
                "metadata",
                "breakerRulesets",
                "staleChannelFlushMs",
                "authTokens",
                "maxS2Sversion",
                "description",
                "useFwdTimezone",
                "dropControlFields",
                "extractMetrics",
                "compress",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeHTTP(str, Enum):
    HTTP = "http"


class CreateInputInputHTTPTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeHTTP
    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    port: float
    r"""Port to listen on"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[TLSSettingsServerSideTypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[ItemsTypeAuthTokensExtTypedDict]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    description: NotRequired[str]


class CreateInputInputHTTP(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeHTTP

    host: str
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    port: float
    r"""Port to listen on"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[TLSSettingsServerSideType] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        None
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = None
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = None
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = None
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = None
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = None
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = None
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = None
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = None
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = None
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = None
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = None
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        None
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = None

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[ItemsTypeAuthTokensExt]], pydantic.Field(alias="authTokensExt")
    ] = None
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "authTokens",
                "tls",
                "maxActiveReq",
                "maxRequestsPerSocket",
                "enableProxyHeader",
                "captureHeaders",
                "activityLogSampleRate",
                "requestTimeout",
                "socketTimeout",
                "keepAliveTimeout",
                "enableHealthCheck",
                "ipAllowlistRegex",
                "ipDenylistRegex",
                "criblAPI",
                "elasticAPI",
                "splunkHecAPI",
                "splunkHecAcks",
                "metadata",
                "authTokensExt",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeMsk(str, Enum):
    MSK = "msk"


class CreateInputInputMskTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeMsk
    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    aws_authentication_method: AuthenticationMethodOptionsS3CollectorConf
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    region: str
    r"""Region where the MSK cluster is located"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    session_timeout: NotRequired[float]
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    kafka_schema_registry: NotRequired[KafkaSchemaRegistryAuthenticationTypeTypedDict]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access MSK"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    tls: NotRequired[TLSSettingsClientSideTypeKafkaSchemaRegistryTypedDict]
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class CreateInputInputMsk(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeMsk

    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    aws_authentication_method: Annotated[
        AuthenticationMethodOptionsS3CollectorConf,
        pydantic.Field(alias="awsAuthenticationMethod"),
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    region: str
    r"""Region where the MSK cluster is located"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = None
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        None
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = None
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = None
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = None
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    kafka_schema_registry: Annotated[
        Optional[KafkaSchemaRegistryAuthenticationType],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = None
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = None
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = None
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = None
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = None
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = None
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptions], pydantic.Field(alias="signatureVersion")
    ] = None
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use Assume Role credentials to access MSK"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    tls: Optional[TLSSettingsClientSideTypeKafkaSchemaRegistry] = None

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = None
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = None
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = None
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "groupId",
                "fromBeginning",
                "sessionTimeout",
                "rebalanceTimeout",
                "heartbeatInterval",
                "metadata",
                "kafkaSchemaRegistry",
                "connectionTimeout",
                "requestTimeout",
                "maxRetries",
                "maxBackOff",
                "initialBackoff",
                "backoffRate",
                "authenticationTimeout",
                "reauthenticationThreshold",
                "awsSecretKey",
                "endpoint",
                "signatureVersion",
                "reuseConnections",
                "rejectUnauthorized",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "tls",
                "autoCommitInterval",
                "autoCommitThreshold",
                "maxBytesPerPartition",
                "maxBytes",
                "maxSocketErrors",
                "description",
                "awsApiKey",
                "awsSecret",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeKafka(str, Enum):
    KAFKA = "kafka"


class CreateInputInputKafkaTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeKafka
    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    kafka_schema_registry: NotRequired[KafkaSchemaRegistryAuthenticationTypeTypedDict]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    sasl: NotRequired[AuthenticationTypeTypedDict]
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""
    tls: NotRequired[TLSSettingsClientSideTypeKafkaSchemaRegistryTypedDict]
    session_timeout: NotRequired[float]
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]


class CreateInputInputKafka(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeKafka

    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = None
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        None
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    kafka_schema_registry: Annotated[
        Optional[KafkaSchemaRegistryAuthenticationType],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = None
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = None
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = None
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = None
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = None
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = None
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    sasl: Optional[AuthenticationType] = None
    r"""Authentication parameters to use when connecting to brokers. Using TLS is highly recommended."""

    tls: Optional[TLSSettingsClientSideTypeKafkaSchemaRegistry] = None

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = None
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = None
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = None
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = None
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = None
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = None
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "groupId",
                "fromBeginning",
                "kafkaSchemaRegistry",
                "connectionTimeout",
                "requestTimeout",
                "maxRetries",
                "maxBackOff",
                "initialBackoff",
                "backoffRate",
                "authenticationTimeout",
                "reauthenticationThreshold",
                "sasl",
                "tls",
                "sessionTimeout",
                "rebalanceTimeout",
                "heartbeatInterval",
                "autoCommitInterval",
                "autoCommitThreshold",
                "maxBytesPerPartition",
                "maxBytes",
                "maxSocketErrors",
                "metadata",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class CreateInputTypeCollection(str, Enum):
    COLLECTION = "collection"


class CreateInputInputCollectionTypedDict(TypedDict):
    id: str
    r"""Unique ID for this input"""
    type: CreateInputTypeCollection
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process results"""
    send_to_routes: NotRequired[bool]
    r"""Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    preprocess: NotRequired[PreprocessTypeSavedJobCollectionInputTypedDict]
    throttle_rate_per_sec: NotRequired[str]
    r"""Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    output: NotRequired[str]
    r"""Destination to send results to"""


class CreateInputInputCollection(BaseModel):
    id: str
    r"""Unique ID for this input"""

    type: CreateInputTypeCollection

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process results"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    preprocess: Optional[PreprocessTypeSavedJobCollectionInput] = None

    throttle_rate_per_sec: Annotated[
        Optional[str], pydantic.Field(alias="throttleRatePerSec")
    ] = None
    r"""Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    output: Optional[str] = None
    r"""Destination to send results to"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "breakerRulesets",
                "staleChannelFlushMs",
                "preprocess",
                "throttleRatePerSec",
                "metadata",
                "output",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


CreateInputRequestTypedDict = TypeAliasType(
    "CreateInputRequestTypedDict",
    Union[
        CreateInputInputCriblTypedDict,
        CreateInputInputKubeEventsTypedDict,
        CreateInputInputDatagenTypedDict,
        CreateInputInputCriblmetricsTypedDict,
        CreateInputInputKubeMetricsTypedDict,
        CreateInputInputCollectionTypedDict,
        CreateInputInputModelDrivenTelemetryTypedDict,
        CreateInputInputWindowsMetricsTypedDict,
        CreateInputInputSystemMetricsTypedDict,
        CreateInputInputSystemStateTypedDict,
        CreateInputInputJournalFilesTypedDict,
        CreateInputInputExecTypedDict,
        CreateInputInputKubeLogsTypedDict,
        CreateInputInputRawUDPTypedDict,
        CreateInputInputSnmpTypedDict,
        CreateInputInputMetricsTypedDict,
        CreateInputInputWinEventLogsTypedDict,
        CreateInputInputCriblTCPTypedDict,
        CreateInputInputNetflowTypedDict,
        CreateInputInputGooglePubsubTypedDict,
        CreateInputInputTcpjsonTypedDict,
        CreateInputInputOffice365ServiceTypedDict,
        CreateInputInputWizTypedDict,
        CreateInputInputFirehoseTypedDict,
        CreateInputInputCriblHTTPTypedDict,
        CreateInputInputOffice365MgmtTypedDict,
        CreateInputInputDatadogAgentTypedDict,
        CreateInputInputTCPTypedDict,
        CreateInputInputSplunkTypedDict,
        CreateInputInputWefTypedDict,
        CreateInputInputAppscopeTypedDict,
        CreateInputInputFileTypedDict,
        CreateInputInputWizWebhookTypedDict,
        CreateInputInputHTTPTypedDict,
        CreateInputInputCriblLakeHTTPTypedDict,
        CreateInputInputHTTPRawTypedDict,
        CreateInputInputAzureBlobTypedDict,
        CreateInputInputZscalerHecTypedDict,
        CreateInputInputSqsTypedDict,
        CreateInputInputCloudflareHecTypedDict,
        CreateInputInputKinesisTypedDict,
        CreateInputInputEventhubTypedDict,
        CreateInputInputConfluentCloudTypedDict,
        CreateInputInputKafkaTypedDict,
        CreateInputInputElasticTypedDict,
        CreateInputInputSplunkHecTypedDict,
        CreateInputInputOffice365MsgTraceTypedDict,
        CreateInputInputPrometheusRwTypedDict,
        CreateInputInputLokiTypedDict,
        CreateInputInputCrowdstrikeTypedDict,
        CreateInputInputOpenTelemetryTypedDict,
        CreateInputInputEdgePrometheusTypedDict,
        CreateInputInputSecurityLakeTypedDict,
        CreateInputInputS3TypedDict,
        CreateInputInputMskTypedDict,
        CreateInputInputPrometheusTypedDict,
        CreateInputInputSplunkSearchTypedDict,
        CreateInputInputS3InventoryTypedDict,
        CreateInputInputGrafanaUnionTypedDict,
        CreateInputInputSyslogUnionTypedDict,
    ],
)
r"""Input object"""


CreateInputRequest = Annotated[
    Union[
        Annotated[CreateInputInputCollection, Tag("collection")],
        Annotated[CreateInputInputKafka, Tag("kafka")],
        Annotated[CreateInputInputMsk, Tag("msk")],
        Annotated[CreateInputInputHTTP, Tag("http")],
        Annotated[CreateInputInputSplunk, Tag("splunk")],
        Annotated[CreateInputInputSplunkSearch, Tag("splunk_search")],
        Annotated[CreateInputInputSplunkHec, Tag("splunk_hec")],
        Annotated[CreateInputInputAzureBlob, Tag("azure_blob")],
        Annotated[CreateInputInputElastic, Tag("elastic")],
        Annotated[CreateInputInputConfluentCloud, Tag("confluent_cloud")],
        Annotated[CreateInputInputGrafanaUnion, Tag("grafana")],
        Annotated[CreateInputInputLoki, Tag("loki")],
        Annotated[CreateInputInputPrometheusRw, Tag("prometheus_rw")],
        Annotated[CreateInputInputPrometheus, Tag("prometheus")],
        Annotated[CreateInputInputEdgePrometheus, Tag("edge_prometheus")],
        Annotated[CreateInputInputOffice365Mgmt, Tag("office365_mgmt")],
        Annotated[CreateInputInputOffice365Service, Tag("office365_service")],
        Annotated[CreateInputInputOffice365MsgTrace, Tag("office365_msg_trace")],
        Annotated[CreateInputInputEventhub, Tag("eventhub")],
        Annotated[CreateInputInputExec, Tag("exec")],
        Annotated[CreateInputInputFirehose, Tag("firehose")],
        Annotated[CreateInputInputGooglePubsub, Tag("google_pubsub")],
        Annotated[CreateInputInputCribl, Tag("cribl")],
        Annotated[CreateInputInputCriblTCP, Tag("cribl_tcp")],
        Annotated[CreateInputInputCriblHTTP, Tag("cribl_http")],
        Annotated[CreateInputInputCriblLakeHTTP, Tag("cribl_lake_http")],
        Annotated[CreateInputInputTcpjson, Tag("tcpjson")],
        Annotated[CreateInputInputSystemMetrics, Tag("system_metrics")],
        Annotated[CreateInputInputSystemState, Tag("system_state")],
        Annotated[CreateInputInputKubeMetrics, Tag("kube_metrics")],
        Annotated[CreateInputInputKubeLogs, Tag("kube_logs")],
        Annotated[CreateInputInputKubeEvents, Tag("kube_events")],
        Annotated[CreateInputInputWindowsMetrics, Tag("windows_metrics")],
        Annotated[CreateInputInputCrowdstrike, Tag("crowdstrike")],
        Annotated[CreateInputInputDatadogAgent, Tag("datadog_agent")],
        Annotated[CreateInputInputDatagen, Tag("datagen")],
        Annotated[CreateInputInputHTTPRaw, Tag("http_raw")],
        Annotated[CreateInputInputKinesis, Tag("kinesis")],
        Annotated[CreateInputInputCriblmetrics, Tag("criblmetrics")],
        Annotated[CreateInputInputMetrics, Tag("metrics")],
        Annotated[CreateInputInputS3, Tag("s3")],
        Annotated[CreateInputInputS3Inventory, Tag("s3_inventory")],
        Annotated[CreateInputInputSnmp, Tag("snmp")],
        Annotated[CreateInputInputOpenTelemetry, Tag("open_telemetry")],
        Annotated[CreateInputInputModelDrivenTelemetry, Tag("model_driven_telemetry")],
        Annotated[CreateInputInputSqs, Tag("sqs")],
        Annotated[CreateInputInputSyslogUnion, Tag("syslog")],
        Annotated[CreateInputInputFile, Tag("file")],
        Annotated[CreateInputInputTCP, Tag("tcp")],
        Annotated[CreateInputInputAppscope, Tag("appscope")],
        Annotated[CreateInputInputWef, Tag("wef")],
        Annotated[CreateInputInputWinEventLogs, Tag("win_event_logs")],
        Annotated[CreateInputInputRawUDP, Tag("raw_udp")],
        Annotated[CreateInputInputJournalFiles, Tag("journal_files")],
        Annotated[CreateInputInputWiz, Tag("wiz")],
        Annotated[CreateInputInputWizWebhook, Tag("wiz_webhook")],
        Annotated[CreateInputInputNetflow, Tag("netflow")],
        Annotated[CreateInputInputSecurityLake, Tag("security_lake")],
        Annotated[CreateInputInputZscalerHec, Tag("zscaler_hec")],
        Annotated[CreateInputInputCloudflareHec, Tag("cloudflare_hec")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""Input object"""


try:
    CreateInputInputCriblHTTP.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputCriblTCP.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputCribl.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputGooglePubsub.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputFirehose.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputExec.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputEventhub.model_rebuild()
except NameError:
    pass
try:
    CreateInputCertOptions.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputOffice365MsgTrace.model_rebuild()
except NameError:
    pass
try:
    ContentConfigOffice365Service.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputOffice365Service.model_rebuild()
except NameError:
    pass
try:
    ContentConfigOffice365Mgmt.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputOffice365Mgmt.model_rebuild()
except NameError:
    pass
try:
    CreateInputPodFilter.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputEdgePrometheus.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputPrometheus.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputPrometheusRw.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputLoki.model_rebuild()
except NameError:
    pass
try:
    CreateInputPrometheusAuth2.model_rebuild()
except NameError:
    pass
try:
    CreateInputLokiAuth2.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputGrafanaGrafana2.model_rebuild()
except NameError:
    pass
try:
    CreateInputPrometheusAuth1.model_rebuild()
except NameError:
    pass
try:
    CreateInputLokiAuth1.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputGrafanaGrafana1.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputConfluentCloud.model_rebuild()
except NameError:
    pass
try:
    ProxyModeElastic.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputElastic.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputAzureBlob.model_rebuild()
except NameError:
    pass
try:
    AuthTokenSplunkHec.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputSplunkHec.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputSplunkSearch.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputSplunk.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputHTTP.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputMsk.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputKafka.model_rebuild()
except NameError:
    pass
try:
    CreateInputInputCollection.model_rebuild()
except NameError:
    pass
