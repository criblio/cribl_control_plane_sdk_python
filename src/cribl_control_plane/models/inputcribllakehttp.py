"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .connectionstype import ConnectionsType, ConnectionsTypeTypedDict
from .metadata1type import Metadata1Type, Metadata1TypeTypedDict
from .pqtype import PqType, PqTypeTypedDict
from .tls2type import Tls2Type, Tls2TypeTypedDict
from cribl_control_plane.types import BaseModel
from enum import Enum
import pydantic
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class InputCriblLakeHTTPType4(str, Enum):
    CRIBL_LAKE_HTTP = "cribl_lake_http"


class InputCriblLakeHTTPMetadatum4TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblLakeHTTPMetadatum4(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SplunkHecMetadata4TypedDict(TypedDict):
    enabled: NotRequired[bool]


class SplunkHecMetadata4(BaseModel):
    enabled: Optional[bool] = None


class ElasticsearchMetadata4TypedDict(TypedDict):
    enabled: NotRequired[bool]


class ElasticsearchMetadata4(BaseModel):
    enabled: Optional[bool] = None


class AuthTokensExt4TypedDict(TypedDict):
    token: str
    description: NotRequired[str]
    metadata: NotRequired[List[InputCriblLakeHTTPMetadatum4TypedDict]]
    r"""Fields to add to events referencing this token"""
    splunk_hec_metadata: NotRequired[SplunkHecMetadata4TypedDict]
    elasticsearch_metadata: NotRequired[ElasticsearchMetadata4TypedDict]


class AuthTokensExt4(BaseModel):
    token: str

    description: Optional[str] = None

    metadata: Optional[List[InputCriblLakeHTTPMetadatum4]] = None
    r"""Fields to add to events referencing this token"""

    splunk_hec_metadata: Annotated[
        Optional[SplunkHecMetadata4], pydantic.Field(alias="splunkHecMetadata")
    ] = None

    elasticsearch_metadata: Annotated[
        Optional[ElasticsearchMetadata4], pydantic.Field(alias="elasticsearchMetadata")
    ] = None


class InputCriblLakeHTTPCriblLakeHTTP4TypedDict(TypedDict):
    type: InputCriblLakeHTTPType4
    pq: PqTypeTypedDict
    port: float
    r"""Port to listen on"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[Tls2TypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[AuthTokensExt4TypedDict]]
    description: NotRequired[str]


class InputCriblLakeHTTPCriblLakeHTTP4(BaseModel):
    type: InputCriblLakeHTTPType4

    pq: PqType

    port: float
    r"""Port to listen on"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[Tls2Type] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = "/cribl"
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = (
        "/elastic"
    )
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExt4]], pydantic.Field(alias="authTokensExt")
    ] = None

    description: Optional[str] = None


class InputCriblLakeHTTPType3(str, Enum):
    CRIBL_LAKE_HTTP = "cribl_lake_http"


class InputCriblLakeHTTPMetadatum3TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblLakeHTTPMetadatum3(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SplunkHecMetadata3TypedDict(TypedDict):
    enabled: NotRequired[bool]


class SplunkHecMetadata3(BaseModel):
    enabled: Optional[bool] = None


class ElasticsearchMetadata3TypedDict(TypedDict):
    enabled: NotRequired[bool]


class ElasticsearchMetadata3(BaseModel):
    enabled: Optional[bool] = None


class AuthTokensExt3TypedDict(TypedDict):
    token: str
    description: NotRequired[str]
    metadata: NotRequired[List[InputCriblLakeHTTPMetadatum3TypedDict]]
    r"""Fields to add to events referencing this token"""
    splunk_hec_metadata: NotRequired[SplunkHecMetadata3TypedDict]
    elasticsearch_metadata: NotRequired[ElasticsearchMetadata3TypedDict]


class AuthTokensExt3(BaseModel):
    token: str

    description: Optional[str] = None

    metadata: Optional[List[InputCriblLakeHTTPMetadatum3]] = None
    r"""Fields to add to events referencing this token"""

    splunk_hec_metadata: Annotated[
        Optional[SplunkHecMetadata3], pydantic.Field(alias="splunkHecMetadata")
    ] = None

    elasticsearch_metadata: Annotated[
        Optional[ElasticsearchMetadata3], pydantic.Field(alias="elasticsearchMetadata")
    ] = None


class InputCriblLakeHTTPCriblLakeHTTP3TypedDict(TypedDict):
    type: InputCriblLakeHTTPType3
    port: float
    r"""Port to listen on"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[Tls2TypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[AuthTokensExt3TypedDict]]
    description: NotRequired[str]


class InputCriblLakeHTTPCriblLakeHTTP3(BaseModel):
    type: InputCriblLakeHTTPType3

    port: float
    r"""Port to listen on"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[Tls2Type] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = "/cribl"
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = (
        "/elastic"
    )
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExt3]], pydantic.Field(alias="authTokensExt")
    ] = None

    description: Optional[str] = None


class InputCriblLakeHTTPType2(str, Enum):
    CRIBL_LAKE_HTTP = "cribl_lake_http"


class InputCriblLakeHTTPMetadatum2TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblLakeHTTPMetadatum2(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SplunkHecMetadata2TypedDict(TypedDict):
    enabled: NotRequired[bool]


class SplunkHecMetadata2(BaseModel):
    enabled: Optional[bool] = None


class ElasticsearchMetadata2TypedDict(TypedDict):
    enabled: NotRequired[bool]


class ElasticsearchMetadata2(BaseModel):
    enabled: Optional[bool] = None


class AuthTokensExt2TypedDict(TypedDict):
    token: str
    description: NotRequired[str]
    metadata: NotRequired[List[InputCriblLakeHTTPMetadatum2TypedDict]]
    r"""Fields to add to events referencing this token"""
    splunk_hec_metadata: NotRequired[SplunkHecMetadata2TypedDict]
    elasticsearch_metadata: NotRequired[ElasticsearchMetadata2TypedDict]


class AuthTokensExt2(BaseModel):
    token: str

    description: Optional[str] = None

    metadata: Optional[List[InputCriblLakeHTTPMetadatum2]] = None
    r"""Fields to add to events referencing this token"""

    splunk_hec_metadata: Annotated[
        Optional[SplunkHecMetadata2], pydantic.Field(alias="splunkHecMetadata")
    ] = None

    elasticsearch_metadata: Annotated[
        Optional[ElasticsearchMetadata2], pydantic.Field(alias="elasticsearchMetadata")
    ] = None


class InputCriblLakeHTTPCriblLakeHTTP2TypedDict(TypedDict):
    type: InputCriblLakeHTTPType2
    connections: List[ConnectionsTypeTypedDict]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    port: float
    r"""Port to listen on"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    pq: NotRequired[PqTypeTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[Tls2TypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[AuthTokensExt2TypedDict]]
    description: NotRequired[str]


class InputCriblLakeHTTPCriblLakeHTTP2(BaseModel):
    type: InputCriblLakeHTTPType2

    connections: List[ConnectionsType]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    port: float
    r"""Port to listen on"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    pq: Optional[PqType] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[Tls2Type] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = "/cribl"
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = (
        "/elastic"
    )
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExt2]], pydantic.Field(alias="authTokensExt")
    ] = None

    description: Optional[str] = None


class InputCriblLakeHTTPType1(str, Enum):
    CRIBL_LAKE_HTTP = "cribl_lake_http"


class InputCriblLakeHTTPMetadatum1TypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class InputCriblLakeHTTPMetadatum1(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)"""


class SplunkHecMetadata1TypedDict(TypedDict):
    enabled: NotRequired[bool]


class SplunkHecMetadata1(BaseModel):
    enabled: Optional[bool] = None


class ElasticsearchMetadata1TypedDict(TypedDict):
    enabled: NotRequired[bool]


class ElasticsearchMetadata1(BaseModel):
    enabled: Optional[bool] = None


class AuthTokensExt1TypedDict(TypedDict):
    token: str
    description: NotRequired[str]
    metadata: NotRequired[List[InputCriblLakeHTTPMetadatum1TypedDict]]
    r"""Fields to add to events referencing this token"""
    splunk_hec_metadata: NotRequired[SplunkHecMetadata1TypedDict]
    elasticsearch_metadata: NotRequired[ElasticsearchMetadata1TypedDict]


class AuthTokensExt1(BaseModel):
    token: str

    description: Optional[str] = None

    metadata: Optional[List[InputCriblLakeHTTPMetadatum1]] = None
    r"""Fields to add to events referencing this token"""

    splunk_hec_metadata: Annotated[
        Optional[SplunkHecMetadata1], pydantic.Field(alias="splunkHecMetadata")
    ] = None

    elasticsearch_metadata: Annotated[
        Optional[ElasticsearchMetadata1], pydantic.Field(alias="elasticsearchMetadata")
    ] = None


class InputCriblLakeHTTPCriblLakeHTTP1TypedDict(TypedDict):
    type: InputCriblLakeHTTPType1
    port: float
    r"""Port to listen on"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    host: NotRequired[str]
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""
    auth_tokens: NotRequired[List[str]]
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""
    tls: NotRequired[Tls2TypeTypedDict]
    max_active_req: NotRequired[float]
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""
    max_requests_per_socket: NotRequired[int]
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""
    enable_proxy_header: NotRequired[bool]
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""
    capture_headers: NotRequired[bool]
    r"""Add request headers to events, in the __headers field"""
    activity_log_sample_rate: NotRequired[float]
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""
    request_timeout: NotRequired[float]
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""
    socket_timeout: NotRequired[float]
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""
    keep_alive_timeout: NotRequired[float]
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""
    enable_health_check: NotRequired[bool]
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""
    ip_allowlist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""
    ip_denylist_regex: NotRequired[str]
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""
    cribl_api: NotRequired[str]
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""
    elastic_api: NotRequired[str]
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""
    splunk_hec_api: NotRequired[str]
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""
    splunk_hec_acks: NotRequired[bool]
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_tokens_ext: NotRequired[List[AuthTokensExt1TypedDict]]
    description: NotRequired[str]


class InputCriblLakeHTTPCriblLakeHTTP1(BaseModel):
    type: InputCriblLakeHTTPType1

    port: float
    r"""Port to listen on"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    host: Optional[str] = "0.0.0.0"
    r"""Address to bind on. Defaults to 0.0.0.0 (all addresses)."""

    auth_tokens: Annotated[Optional[List[str]], pydantic.Field(alias="authTokens")] = (
        None
    )
    r"""Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted."""

    tls: Optional[Tls2Type] = None

    max_active_req: Annotated[Optional[float], pydantic.Field(alias="maxActiveReq")] = (
        256
    )
    r"""Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput."""

    max_requests_per_socket: Annotated[
        Optional[int], pydantic.Field(alias="maxRequestsPerSocket")
    ] = 0
    r"""Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited)."""

    enable_proxy_header: Annotated[
        Optional[bool], pydantic.Field(alias="enableProxyHeader")
    ] = False
    r"""Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction."""

    capture_headers: Annotated[
        Optional[bool], pydantic.Field(alias="captureHeaders")
    ] = False
    r"""Add request headers to events, in the __headers field"""

    activity_log_sample_rate: Annotated[
        Optional[float], pydantic.Field(alias="activityLogSampleRate")
    ] = 100
    r"""How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 0
    r"""How long to wait for an incoming request to complete before aborting it. Use 0 to disable."""

    socket_timeout: Annotated[
        Optional[float], pydantic.Field(alias="socketTimeout")
    ] = 0
    r"""How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0."""

    keep_alive_timeout: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTimeout")
    ] = 5
    r"""After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes)."""

    enable_health_check: Annotated[
        Optional[bool], pydantic.Field(alias="enableHealthCheck")
    ] = False
    r"""Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy"""

    ip_allowlist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipAllowlistRegex")
    ] = "/.*/"
    r"""Messages from matched IP addresses will be processed, unless also matched by the denylist"""

    ip_denylist_regex: Annotated[
        Optional[str], pydantic.Field(alias="ipDenylistRegex")
    ] = "/^$/"
    r"""Messages from matched IP addresses will be ignored. This takes precedence over the allowlist."""

    cribl_api: Annotated[Optional[str], pydantic.Field(alias="criblAPI")] = "/cribl"
    r"""Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable."""

    elastic_api: Annotated[Optional[str], pydantic.Field(alias="elasticAPI")] = (
        "/elastic"
    )
    r"""Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable."""

    splunk_hec_api: Annotated[Optional[str], pydantic.Field(alias="splunkHecAPI")] = (
        "/services/collector"
    )
    r"""Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable."""

    splunk_hec_acks: Annotated[
        Optional[bool], pydantic.Field(alias="splunkHecAcks")
    ] = False

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_tokens_ext: Annotated[
        Optional[List[AuthTokensExt1]], pydantic.Field(alias="authTokensExt")
    ] = None

    description: Optional[str] = None


InputCriblLakeHTTPTypedDict = TypeAliasType(
    "InputCriblLakeHTTPTypedDict",
    Union[
        InputCriblLakeHTTPCriblLakeHTTP1TypedDict,
        InputCriblLakeHTTPCriblLakeHTTP2TypedDict,
        InputCriblLakeHTTPCriblLakeHTTP3TypedDict,
        InputCriblLakeHTTPCriblLakeHTTP4TypedDict,
    ],
)


InputCriblLakeHTTP = TypeAliasType(
    "InputCriblLakeHTTP",
    Union[
        InputCriblLakeHTTPCriblLakeHTTP1,
        InputCriblLakeHTTPCriblLakeHTTP2,
        InputCriblLakeHTTPCriblLakeHTTP3,
        InputCriblLakeHTTPCriblLakeHTTP4,
    ],
)
