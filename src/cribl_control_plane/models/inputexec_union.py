"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .connectionstype import ConnectionsType, ConnectionsTypeTypedDict
from .metadata1type import Metadata1Type, Metadata1TypeTypedDict
from .pqtype import PqType, PqTypeTypedDict
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class ScheduleTypeEnum6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExecType6(str, Enum):
    EXEC = "exec"


class InputExec6TypedDict(TypedDict):
    type: InputExecType6
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    schedule_type: NotRequired[ScheduleTypeEnum6]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec6(BaseModel):
    type: InputExecType6

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    schedule_type: Annotated[
        Annotated[
            Optional[ScheduleTypeEnum6], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleTypeEnum6.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeEnum6(value)
            except ValueError:
                return value
        return value


class ScheduleTypeEnum5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExecType5(str, Enum):
    EXEC = "exec"


class InputExec5TypedDict(TypedDict):
    type: InputExecType5
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    schedule_type: NotRequired[ScheduleTypeEnum5]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec5(BaseModel):
    type: InputExecType5

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    schedule_type: Annotated[
        Annotated[
            Optional[ScheduleTypeEnum5], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleTypeEnum5.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeEnum5(value)
            except ValueError:
                return value
        return value


class InputExecType4(str, Enum):
    EXEC = "exec"


class ScheduleTypeEnum4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExec4TypedDict(TypedDict):
    type: InputExecType4
    pq: PqTypeTypedDict
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    schedule_type: NotRequired[ScheduleTypeEnum4]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec4(BaseModel):
    type: InputExecType4

    pq: PqType

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    schedule_type: Annotated[
        Annotated[
            Optional[ScheduleTypeEnum4], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleTypeEnum4.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeEnum4(value)
            except ValueError:
                return value
        return value


class InputExecType3(str, Enum):
    EXEC = "exec"


class ScheduleTypeEnum3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExec3TypedDict(TypedDict):
    type: InputExecType3
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    schedule_type: NotRequired[ScheduleTypeEnum3]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec3(BaseModel):
    type: InputExecType3

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    schedule_type: Annotated[
        Annotated[
            Optional[ScheduleTypeEnum3], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleTypeEnum3.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeEnum3(value)
            except ValueError:
                return value
        return value


class InputExecType2(str, Enum):
    EXEC = "exec"


class ScheduleTypeEnum2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExec2TypedDict(TypedDict):
    type: InputExecType2
    connections: List[ConnectionsTypeTypedDict]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    pq: NotRequired[PqTypeTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    schedule_type: NotRequired[ScheduleTypeEnum2]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec2(BaseModel):
    type: InputExecType2

    connections: List[ConnectionsType]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    pq: Optional[PqType] = None

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    schedule_type: Annotated[
        Annotated[
            Optional[ScheduleTypeEnum2], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleTypeEnum2.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeEnum2(value)
            except ValueError:
                return value
        return value


class InputExecType1(str, Enum):
    EXEC = "exec"


class ScheduleTypeEnum1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    INTERVAL = "interval"
    CRON_SCHEDULE = "cronSchedule"


class InputExec1TypedDict(TypedDict):
    type: InputExecType1
    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    retries: NotRequired[float]
    r"""Maximum number of retry attempts in the event that the command fails"""
    schedule_type: NotRequired[ScheduleTypeEnum1]
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    interval: NotRequired[float]
    r"""Interval between command executions in seconds."""
    cron_schedule: NotRequired[str]
    r"""Cron schedule to execute the command on."""


class InputExec1(BaseModel):
    type: InputExecType1

    command: str
    r"""Command to execute; supports Bourne shell (or CMD on Windows) syntax"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    retries: Optional[float] = 10
    r"""Maximum number of retry attempts in the event that the command fails"""

    schedule_type: Annotated[
        Annotated[
            Optional[ScheduleTypeEnum1], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scheduleType"),
    ] = ScheduleTypeEnum1.INTERVAL
    r"""Select a schedule type; either an interval (in seconds) or a cron-style schedule."""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    interval: Optional[float] = 60
    r"""Interval between command executions in seconds."""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = (
        "* * * * *"
    )
    r"""Cron schedule to execute the command on."""

    @field_serializer("schedule_type")
    def serialize_schedule_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeEnum1(value)
            except ValueError:
                return value
        return value


InputExecUnionTypedDict = TypeAliasType(
    "InputExecUnionTypedDict",
    Union[
        InputExec1TypedDict,
        InputExec2TypedDict,
        InputExec3TypedDict,
        InputExec4TypedDict,
        InputExec5TypedDict,
        InputExec6TypedDict,
    ],
)


InputExecUnion = TypeAliasType(
    "InputExecUnion",
    Union[InputExec1, InputExec2, InputExec3, InputExec4, InputExec5, InputExec6],
)
