"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .connectionstype import ConnectionsType, ConnectionsTypeTypedDict
from .metadata1type import Metadata1Type, Metadata1TypeTypedDict
from .pqtype import PqType, PqTypeTypedDict
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class InputFileType8(str, Enum):
    FILE = "file"


class InputFileMode8(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFile8TypedDict(TypedDict):
    type: InputFileType8
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    mode: NotRequired[InputFileMode8]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile8(BaseModel):
    type: InputFileType8

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    mode: Annotated[
        Optional[InputFileMode8], PlainValidator(validate_open_enum(False))
    ] = InputFileMode8.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode8(value)
            except ValueError:
                return value
        return value


class InputFileType7(str, Enum):
    FILE = "file"


class InputFileMode7(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFile7TypedDict(TypedDict):
    type: InputFileType7
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    mode: NotRequired[InputFileMode7]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile7(BaseModel):
    type: InputFileType7

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    mode: Annotated[
        Optional[InputFileMode7], PlainValidator(validate_open_enum(False))
    ] = InputFileMode7.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode7(value)
            except ValueError:
                return value
        return value


class InputFileMode6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFileType6(str, Enum):
    FILE = "file"


class InputFile6TypedDict(TypedDict):
    type: InputFileType6
    path: str
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: float
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    mode: NotRequired[InputFileMode6]
    r"""Choose how to discover files to monitor"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile6(BaseModel):
    type: InputFileType6

    path: str
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: float
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    mode: Annotated[
        Optional[InputFileMode6], PlainValidator(validate_open_enum(False))
    ] = InputFileMode6.MANUAL
    r"""Choose how to discover files to monitor"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode6(value)
            except ValueError:
                return value
        return value


class InputFileMode5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFileType5(str, Enum):
    FILE = "file"


class InputFile5TypedDict(TypedDict):
    type: InputFileType5
    mode: NotRequired[InputFileMode5]
    r"""Choose how to discover files to monitor"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile5(BaseModel):
    type: InputFileType5

    mode: Annotated[
        Optional[InputFileMode5], PlainValidator(validate_open_enum(False))
    ] = InputFileMode5.MANUAL
    r"""Choose how to discover files to monitor"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode5(value)
            except ValueError:
                return value
        return value


class InputFileType4(str, Enum):
    FILE = "file"


class InputFileMode4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFile4TypedDict(TypedDict):
    type: InputFileType4
    pq: PqTypeTypedDict
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    mode: NotRequired[InputFileMode4]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile4(BaseModel):
    type: InputFileType4

    pq: PqType

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    mode: Annotated[
        Optional[InputFileMode4], PlainValidator(validate_open_enum(False))
    ] = InputFileMode4.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode4(value)
            except ValueError:
                return value
        return value


class InputFileType3(str, Enum):
    FILE = "file"


class InputFileMode3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFile3TypedDict(TypedDict):
    type: InputFileType3
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    mode: NotRequired[InputFileMode3]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile3(BaseModel):
    type: InputFileType3

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    mode: Annotated[
        Optional[InputFileMode3], PlainValidator(validate_open_enum(False))
    ] = InputFileMode3.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode3(value)
            except ValueError:
                return value
        return value


class InputFileType2(str, Enum):
    FILE = "file"


class InputFileMode2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFile2TypedDict(TypedDict):
    type: InputFileType2
    connections: List[ConnectionsTypeTypedDict]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    pq: NotRequired[PqTypeTypedDict]
    mode: NotRequired[InputFileMode2]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile2(BaseModel):
    type: InputFileType2

    connections: List[ConnectionsType]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    pq: Optional[PqType] = None

    mode: Annotated[
        Optional[InputFileMode2], PlainValidator(validate_open_enum(False))
    ] = InputFileMode2.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode2(value)
            except ValueError:
                return value
        return value


class InputFileType1(str, Enum):
    FILE = "file"


class InputFileMode1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Choose how to discover files to monitor"""

    # Manual
    MANUAL = "manual"
    # Auto
    AUTO = "auto"


class InputFile1TypedDict(TypedDict):
    type: InputFileType1
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    mode: NotRequired[InputFileMode1]
    r"""Choose how to discover files to monitor"""
    interval: NotRequired[float]
    r"""Time, in seconds, between scanning for files"""
    filenames: NotRequired[List[str]]
    r"""The full path of discovered files are matched against this wildcard list"""
    filter_archived_files: NotRequired[bool]
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""
    tail_only: NotRequired[bool]
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""
    idle_timeout: NotRequired[float]
    r"""Time, in seconds, before an idle file is closed"""
    min_age_dur: NotRequired[str]
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""
    max_age_dur: NotRequired[str]
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""
    check_file_mod_time: NotRequired[bool]
    r"""Skip files with modification times earlier than the maximum age duration"""
    force_text: NotRequired[bool]
    r"""Forces files containing binary data to be streamed as text"""
    hash_len: NotRequired[float]
    r"""Length of file header bytes to use in hash for unique file identification"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    description: NotRequired[str]
    path: NotRequired[str]
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""
    depth: NotRequired[float]
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""
    suppress_missing_path_errors: NotRequired[bool]
    delete_files: NotRequired[bool]
    r"""Delete files after they have been collected"""
    include_unidentifiable_binary: NotRequired[bool]
    r"""Stream binary files as Base64-encoded chunks."""


class InputFile1(BaseModel):
    type: InputFileType1

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    mode: Annotated[
        Optional[InputFileMode1], PlainValidator(validate_open_enum(False))
    ] = InputFileMode1.MANUAL
    r"""Choose how to discover files to monitor"""

    interval: Optional[float] = 10
    r"""Time, in seconds, between scanning for files"""

    filenames: Optional[List[str]] = None
    r"""The full path of discovered files are matched against this wildcard list"""

    filter_archived_files: Annotated[
        Optional[bool], pydantic.Field(alias="filterArchivedFiles")
    ] = False
    r"""Apply filename allowlist to file entries in archive file types, like tar or zip."""

    tail_only: Annotated[Optional[bool], pydantic.Field(alias="tailOnly")] = True
    r"""Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head."""

    idle_timeout: Annotated[Optional[float], pydantic.Field(alias="idleTimeout")] = 300
    r"""Time, in seconds, before an idle file is closed"""

    min_age_dur: Annotated[Optional[str], pydantic.Field(alias="minAgeDur")] = None
    r"""The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters."""

    max_age_dur: Annotated[Optional[str], pydantic.Field(alias="maxAgeDur")] = None
    r"""The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with \"Check file modification times\". Leave empty to apply no age filters."""

    check_file_mod_time: Annotated[
        Optional[bool], pydantic.Field(alias="checkFileModTime")
    ] = False
    r"""Skip files with modification times earlier than the maximum age duration"""

    force_text: Annotated[Optional[bool], pydantic.Field(alias="forceText")] = False
    r"""Forces files containing binary data to be streamed as text"""

    hash_len: Annotated[Optional[float], pydantic.Field(alias="hashLen")] = 256
    r"""Length of file header bytes to use in hash for unique file identification"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    description: Optional[str] = None

    path: Optional[str] = None
    r"""Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/."""

    depth: Optional[float] = None
    r"""Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth."""

    suppress_missing_path_errors: Annotated[
        Optional[bool], pydantic.Field(alias="suppressMissingPathErrors")
    ] = False

    delete_files: Annotated[Optional[bool], pydantic.Field(alias="deleteFiles")] = False
    r"""Delete files after they have been collected"""

    include_unidentifiable_binary: Annotated[
        Optional[bool], pydantic.Field(alias="includeUnidentifiableBinary")
    ] = False
    r"""Stream binary files as Base64-encoded chunks."""

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.InputFileMode1(value)
            except ValueError:
                return value
        return value


InputFileUnionTypedDict = TypeAliasType(
    "InputFileUnionTypedDict",
    Union[
        InputFile1TypedDict,
        InputFile2TypedDict,
        InputFile3TypedDict,
        InputFile4TypedDict,
        InputFile5TypedDict,
        InputFile6TypedDict,
        InputFile7TypedDict,
        InputFile8TypedDict,
    ],
)


InputFileUnion = TypeAliasType(
    "InputFileUnion",
    Union[
        InputFile1,
        InputFile2,
        InputFile3,
        InputFile4,
        InputFile5,
        InputFile6,
        InputFile7,
        InputFile8,
    ],
)
