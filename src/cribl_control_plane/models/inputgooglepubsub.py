"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .awsauthenticationmethodoptions import AwsAuthenticationMethodOptions
from .connectionstype import ConnectionsType, ConnectionsTypeTypedDict
from .metadata1type import Metadata1Type, Metadata1TypeTypedDict
from .pqtype import PqType, PqTypeTypedDict
from .typegooglepubsuboption import TypeGooglePubsubOption
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class InputGooglePubsubGooglePubsub9TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub9(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub8TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub8(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub7TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    secret: str
    r"""Select or create a stored text secret"""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub7(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    secret: str
    r"""Select or create a stored text secret"""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub6TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    service_account_credentials: str
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub6(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    service_account_credentials: Annotated[
        str, pydantic.Field(alias="serviceAccountCredentials")
    ]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub5TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub5(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub4TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    pq: PqTypeTypedDict
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub4(BaseModel):
    type: TypeGooglePubsubOption

    pq: PqType

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub3TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub3(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub2TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    connections: List[ConnectionsTypeTypedDict]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub2(BaseModel):
    type: TypeGooglePubsubOption

    connections: List[ConnectionsType]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


class InputGooglePubsubGooglePubsub1TypedDict(TypedDict):
    type: TypeGooglePubsubOption
    subscription_name: str
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    topic_name: NotRequired[str]
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""
    monitor_subscription: NotRequired[bool]
    r"""Use when the subscription is not created by this Source and topic is not known"""
    create_topic: NotRequired[bool]
    r"""Create topic if it does not exist"""
    create_subscription: NotRequired[bool]
    r"""Create subscription if it does not exist"""
    region: NotRequired[str]
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""
    google_auth_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    max_backlog: NotRequired[float]
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""
    concurrency: NotRequired[float]
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""
    request_timeout: NotRequired[float]
    r"""Pull request timeout, in milliseconds"""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    ordered_delivery: NotRequired[bool]
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""


class InputGooglePubsubGooglePubsub1(BaseModel):
    type: TypeGooglePubsubOption

    subscription_name: Annotated[str, pydantic.Field(alias="subscriptionName")]
    r"""ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    topic_name: Annotated[Optional[str], pydantic.Field(alias="topicName")] = "cribl"
    r"""ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered."""

    monitor_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="monitorSubscription")
    ] = False
    r"""Use when the subscription is not created by this Source and topic is not known"""

    create_topic: Annotated[Optional[bool], pydantic.Field(alias="createTopic")] = False
    r"""Create topic if it does not exist"""

    create_subscription: Annotated[
        Optional[bool], pydantic.Field(alias="createSubscription")
    ] = True
    r"""Create subscription if it does not exist"""

    region: Optional[str] = None
    r"""Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy."""

    google_auth_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="googleAuthMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    secret: Optional[str] = None
    r"""Select or create a stored text secret"""

    max_backlog: Annotated[Optional[float], pydantic.Field(alias="maxBacklog")] = 1000
    r"""If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events"""

    concurrency: Optional[float] = 5
    r"""How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5."""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = 60000
    r"""Pull request timeout, in milliseconds"""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    ordered_delivery: Annotated[
        Optional[bool], pydantic.Field(alias="orderedDelivery")
    ] = False
    r"""Receive events in the order they were added to the queue. The process sending events must have ordering enabled."""

    @field_serializer("google_auth_method")
    def serialize_google_auth_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value


InputGooglePubsubTypedDict = TypeAliasType(
    "InputGooglePubsubTypedDict",
    Union[
        InputGooglePubsubGooglePubsub1TypedDict,
        InputGooglePubsubGooglePubsub2TypedDict,
        InputGooglePubsubGooglePubsub3TypedDict,
        InputGooglePubsubGooglePubsub4TypedDict,
        InputGooglePubsubGooglePubsub5TypedDict,
        InputGooglePubsubGooglePubsub6TypedDict,
        InputGooglePubsubGooglePubsub7TypedDict,
        InputGooglePubsubGooglePubsub8TypedDict,
        InputGooglePubsubGooglePubsub9TypedDict,
    ],
)


InputGooglePubsub = TypeAliasType(
    "InputGooglePubsub",
    Union[
        InputGooglePubsubGooglePubsub1,
        InputGooglePubsubGooglePubsub2,
        InputGooglePubsubGooglePubsub3,
        InputGooglePubsubGooglePubsub4,
        InputGooglePubsubGooglePubsub5,
        InputGooglePubsubGooglePubsub6,
        InputGooglePubsubGooglePubsub7,
        InputGooglePubsubGooglePubsub8,
        InputGooglePubsubGooglePubsub9,
    ],
)
