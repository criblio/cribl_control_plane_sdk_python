"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .awsauthenticationmethodoptions import AwsAuthenticationMethodOptions
from .connectionstype import ConnectionsType, ConnectionsTypeTypedDict
from .metadata1type import Metadata1Type, Metadata1TypeTypedDict
from .pqtype import PqType, PqTypeTypedDict
from .signatureversionoptions import SignatureVersionOptions
from .typekinesisoption import TypeKinesisOption
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class ShardIteratorStart7(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat7(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing7(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis7TypedDict(TypedDict):
    type: TypeKinesisOption
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    aws_secret: str
    r"""Select or create a stored secret that references your access key and secret key"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart7]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat7]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing7]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]


class InputKinesisKinesis7(BaseModel):
    type: TypeKinesisOption

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    aws_secret: Annotated[str, pydantic.Field(alias="awsSecret")]
    r"""Select or create a stored secret that references your access key and secret key"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart7], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart7.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat7], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat7.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing7], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing7.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart7(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat7(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing7(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis6TypedDict(TypedDict):
    type: TypeKinesisOption
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    aws_api_key: str
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart6]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat6]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing6]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesisKinesis6(BaseModel):
    type: TypeKinesisOption

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    aws_api_key: Annotated[str, pydantic.Field(alias="awsApiKey")]

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart6], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart6.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat6], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat6.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing6], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing6.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart6(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat6(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing6(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis5TypedDict(TypedDict):
    type: TypeKinesisOption
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart5]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat5]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing5]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesisKinesis5(BaseModel):
    type: TypeKinesisOption

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart5], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart5.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat5], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat5.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing5], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing5.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart5(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat5(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing5(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis4TypedDict(TypedDict):
    type: TypeKinesisOption
    pq: PqTypeTypedDict
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart4]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat4]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing4]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesisKinesis4(BaseModel):
    type: TypeKinesisOption

    pq: PqType

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart4], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart4.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat4], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat4.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing4], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing4.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart4(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat4(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing4(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis3TypedDict(TypedDict):
    type: TypeKinesisOption
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart3]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat3]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing3]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesisKinesis3(BaseModel):
    type: TypeKinesisOption

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart3], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart3.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat3], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat3.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing3], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing3.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart3(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat3(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing3(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis2TypedDict(TypedDict):
    type: TypeKinesisOption
    connections: List[ConnectionsTypeTypedDict]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    pq: NotRequired[PqTypeTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart2]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat2]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing2]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesisKinesis2(BaseModel):
    type: TypeKinesisOption

    connections: List[ConnectionsType]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    pq: Optional[PqType] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart2], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart2.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat2], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat2.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing2], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing2.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart2(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat2(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing2(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class ShardIteratorStart1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Location at which to start reading a shard for the first time"""

    # Earliest record
    TRIM_HORIZON = "TRIM_HORIZON"
    # Latest record
    LATEST = "LATEST"


class RecordDataFormat1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    # Cribl
    CRIBL = "cribl"
    # Newline JSON
    NDJSON = "ndjson"
    # Cloudwatch Logs
    CLOUDWATCH = "cloudwatch"
    # Event per line
    LINE = "line"


class ShardLoadBalancing1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    # Consistent Hashing
    CONSISTENT_HASHING = "ConsistentHashing"
    # Round Robin
    ROUND_ROBIN = "RoundRobin"


class InputKinesisKinesis1TypedDict(TypedDict):
    type: TypeKinesisOption
    stream_name: str
    r"""Kinesis Data Stream to read data from"""
    region: str
    r"""Region where the Kinesis stream is located"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    service_interval: NotRequired[float]
    r"""Time interval in minutes between consecutive service calls"""
    shard_expr: NotRequired[str]
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""
    shard_iterator_type: NotRequired[ShardIteratorStart1]
    r"""Location at which to start reading a shard for the first time"""
    payload_format: NotRequired[RecordDataFormat1]
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""
    get_records_limit: NotRequired[float]
    r"""Maximum number of records per getRecords call"""
    get_records_limit_total: NotRequired[float]
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""
    load_balancing_algorithm: NotRequired[ShardLoadBalancing1]
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    verify_kpl_check_sums: NotRequired[bool]
    r"""Verify Kinesis Producer Library (KPL) event checksums"""
    avoid_duplicates: NotRequired[bool]
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""


class InputKinesisKinesis1(BaseModel):
    type: TypeKinesisOption

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis Data Stream to read data from"""

    region: str
    r"""Region where the Kinesis stream is located"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    service_interval: Annotated[
        Optional[float], pydantic.Field(alias="serviceInterval")
    ] = 1
    r"""Time interval in minutes between consecutive service calls"""

    shard_expr: Annotated[Optional[str], pydantic.Field(alias="shardExpr")] = "true"
    r"""A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed."""

    shard_iterator_type: Annotated[
        Annotated[
            Optional[ShardIteratorStart1], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="shardIteratorType"),
    ] = ShardIteratorStart1.TRIM_HORIZON
    r"""Location at which to start reading a shard for the first time"""

    payload_format: Annotated[
        Annotated[
            Optional[RecordDataFormat1], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="payloadFormat"),
    ] = RecordDataFormat1.CRIBL
    r"""Format of data inside the Kinesis Stream records. Gzip compression is automatically detected."""

    get_records_limit: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimit")
    ] = 5000
    r"""Maximum number of records per getRecords call"""

    get_records_limit_total: Annotated[
        Optional[float], pydantic.Field(alias="getRecordsLimitTotal")
    ] = 20000
    r"""Maximum number of records, across all shards, to pull down at once per Worker Process"""

    load_balancing_algorithm: Annotated[
        Annotated[
            Optional[ShardLoadBalancing1], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="loadBalancingAlgorithm"),
    ] = ShardLoadBalancing1.CONSISTENT_HASHING
    r"""The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    verify_kpl_check_sums: Annotated[
        Optional[bool], pydantic.Field(alias="verifyKPLCheckSums")
    ] = False
    r"""Verify Kinesis Producer Library (KPL) event checksums"""

    avoid_duplicates: Annotated[
        Optional[bool], pydantic.Field(alias="avoidDuplicates")
    ] = False
    r"""When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    @field_serializer("shard_iterator_type")
    def serialize_shard_iterator_type(self, value):
        if isinstance(value, str):
            try:
                return models.ShardIteratorStart1(value)
            except ValueError:
                return value
        return value

    @field_serializer("payload_format")
    def serialize_payload_format(self, value):
        if isinstance(value, str):
            try:
                return models.RecordDataFormat1(value)
            except ValueError:
                return value
        return value

    @field_serializer("load_balancing_algorithm")
    def serialize_load_balancing_algorithm(self, value):
        if isinstance(value, str):
            try:
                return models.ShardLoadBalancing1(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


InputKinesisTypedDict = TypeAliasType(
    "InputKinesisTypedDict",
    Union[
        InputKinesisKinesis1TypedDict,
        InputKinesisKinesis2TypedDict,
        InputKinesisKinesis3TypedDict,
        InputKinesisKinesis4TypedDict,
        InputKinesisKinesis5TypedDict,
        InputKinesisKinesis6TypedDict,
        InputKinesisKinesis7TypedDict,
    ],
)


InputKinesis = TypeAliasType(
    "InputKinesis",
    Union[
        InputKinesisKinesis1,
        InputKinesisKinesis2,
        InputKinesisKinesis3,
        InputKinesisKinesis4,
        InputKinesisKinesis5,
        InputKinesisKinesis6,
        InputKinesisKinesis7,
    ],
)
