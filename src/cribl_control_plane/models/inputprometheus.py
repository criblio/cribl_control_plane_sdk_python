"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authtype2options import AuthType2Options
from .awsauthenticationmethodoptions import AwsAuthenticationMethodOptions
from .connectionstype import ConnectionsType, ConnectionsTypeTypedDict
from .logleveloptions import LogLevelOptions
from .metadata1type import Metadata1Type, Metadata1TypeTypedDict
from .pqtype import PqType, PqTypeTypedDict
from .recordtypeoptions import RecordTypeOptions
from .scrapeprotocoloptions import ScrapeProtocolOptions
from .searchfiltertype import SearchFilterType, SearchFilterTypeTypedDict
from .signatureversionoptions import SignatureVersionOptions
from .typeprometheusoption import TypePrometheusOption
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class InputPrometheusDiscoveryType9(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus9TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    credentials_secret: str
    r"""Select or create a secret that references your credentials"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType9]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""


class InputPrometheusPrometheus9(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    credentials_secret: Annotated[str, pydantic.Field(alias="credentialsSecret")]
    r"""Select or create a secret that references your credentials"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType9],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType9.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType9(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType8(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus8TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    username: str
    r"""Username for Prometheus Basic authentication"""
    password: str
    r"""Password for Prometheus Basic authentication"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType8]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus8(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    username: str
    r"""Username for Prometheus Basic authentication"""

    password: str
    r"""Password for Prometheus Basic authentication"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType8],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType8.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType8(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType7(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus7TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    search_filter: List[SearchFilterTypeTypedDict]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: str
    region: str
    r"""Region where the EC2 is located"""
    endpoint: str
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    assume_role_arn: str
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: str
    r"""External ID to use when assuming role"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType7]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus7(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    search_filter: Annotated[
        List[SearchFilterType], pydantic.Field(alias="searchFilter")
    ]
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[str, pydantic.Field(alias="awsSecretKey")]

    region: str
    r"""Region where the EC2 is located"""

    endpoint: str
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    assume_role_arn: Annotated[str, pydantic.Field(alias="assumeRoleArn")]
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        str, pydantic.Field(alias="assumeRoleExternalId")
    ]
    r"""External ID to use when assuming role"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType7],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType7.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType7(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus6TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    name_list: List[str]
    r"""List of DNS names to resolve"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType6]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus6(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    name_list: Annotated[List[str], pydantic.Field(alias="nameList")]
    r"""List of DNS names to resolve"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType6],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType6.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType6(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus5TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    target_list: List[str]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    discovery_type: NotRequired[InputPrometheusDiscoveryType5]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus5(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    target_list: Annotated[List[str], pydantic.Field(alias="targetList")]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType5],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType5.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType5(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus4TypedDict(TypedDict):
    type: TypePrometheusOption
    pq: PqTypeTypedDict
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType4]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus4(BaseModel):
    type: TypePrometheusOption

    pq: PqType

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType4],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType4.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType4(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus3TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType3]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus3(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType3],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType3.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType3(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus2TypedDict(TypedDict):
    type: TypePrometheusOption
    connections: List[ConnectionsTypeTypedDict]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType2]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus2(BaseModel):
    type: TypePrometheusOption

    connections: List[ConnectionsType]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType2.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType2(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class InputPrometheusDiscoveryType1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    # Static
    STATIC = "static"
    # DNS
    DNS = "dns"
    # AWS EC2
    EC2 = "ec2"


class InputPrometheusPrometheus1TypedDict(TypedDict):
    type: TypePrometheusOption
    log_level: LogLevelOptions
    r"""Collector runtime log level (verbosity)"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ConnectionsTypeTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    dimension_list: NotRequired[List[str]]
    r"""Other dimensions to include in events"""
    discovery_type: NotRequired[InputPrometheusDiscoveryType1]
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""
    interval: NotRequired[float]
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[Metadata1TypeTypedDict]]
    r"""Fields to add to events from this input"""
    auth_type: NotRequired[AuthType2Options]
    r"""Enter credentials directly, or select a stored secret"""
    description: NotRequired[str]
    target_list: NotRequired[List[str]]
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""
    record_type: NotRequired[RecordTypeOptions]
    r"""DNS Record type to resolve"""
    scrape_port: NotRequired[float]
    r"""The port number in the metrics URL for discovered targets."""
    name_list: NotRequired[List[str]]
    r"""List of DNS names to resolve"""
    scrape_protocol: NotRequired[ScrapeProtocolOptions]
    r"""Protocol to use when collecting metrics"""
    scrape_path: NotRequired[str]
    r"""Path to use when collecting metrics from discovered targets"""
    aws_authentication_method: NotRequired[AwsAuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    use_public_ip: NotRequired[bool]
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""
    search_filter: NotRequired[List[SearchFilterTypeTypedDict]]
    r"""EC2 Instance Search Filter"""
    aws_secret_key: NotRequired[str]
    region: NotRequired[str]
    r"""Region where the EC2 is located"""
    endpoint: NotRequired[str]
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access EC2"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    username: NotRequired[str]
    r"""Username for Prometheus Basic authentication"""
    password: NotRequired[str]
    r"""Password for Prometheus Basic authentication"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""


class InputPrometheusPrometheus1(BaseModel):
    type: TypePrometheusOption

    log_level: Annotated[
        Annotated[LogLevelOptions, PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logLevel"),
    ]
    r"""Collector runtime log level (verbosity)"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ConnectionsType]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    dimension_list: Annotated[
        Optional[List[str]], pydantic.Field(alias="dimensionList")
    ] = None
    r"""Other dimensions to include in events"""

    discovery_type: Annotated[
        Annotated[
            Optional[InputPrometheusDiscoveryType1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="discoveryType"),
    ] = InputPrometheusDiscoveryType1.STATIC
    r"""Target discovery mechanism. Use static to manually enter a list of targets."""

    interval: Optional[float] = 15
    r"""How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail."""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = 30
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = 3
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[Metadata1Type]] = None
    r"""Fields to add to events from this input"""

    auth_type: Annotated[
        Annotated[
            Optional[AuthType2Options], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="authType"),
    ] = AuthType2Options.MANUAL
    r"""Enter credentials directly, or select a stored secret"""

    description: Optional[str] = None

    target_list: Annotated[Optional[List[str]], pydantic.Field(alias="targetList")] = (
        None
    )
    r"""List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'."""

    record_type: Annotated[
        Annotated[
            Optional[RecordTypeOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="recordType"),
    ] = RecordTypeOptions.SRV
    r"""DNS Record type to resolve"""

    scrape_port: Annotated[Optional[float], pydantic.Field(alias="scrapePort")] = 9090
    r"""The port number in the metrics URL for discovered targets."""

    name_list: Annotated[Optional[List[str]], pydantic.Field(alias="nameList")] = None
    r"""List of DNS names to resolve"""

    scrape_protocol: Annotated[
        Annotated[
            Optional[ScrapeProtocolOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="scrapeProtocol"),
    ] = ScrapeProtocolOptions.HTTP
    r"""Protocol to use when collecting metrics"""

    scrape_path: Annotated[Optional[str], pydantic.Field(alias="scrapePath")] = (
        "/metrics"
    )
    r"""Path to use when collecting metrics from discovered targets"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AwsAuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AwsAuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    use_public_ip: Annotated[Optional[bool], pydantic.Field(alias="usePublicIp")] = True
    r"""Use public IP address for discovered targets. Set to false if the private IP address should be used."""

    search_filter: Annotated[
        Optional[List[SearchFilterType]], pydantic.Field(alias="searchFilter")
    ] = None
    r"""EC2 Instance Search Filter"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    region: Optional[str] = None
    r"""Region where the EC2 is located"""

    endpoint: Optional[str] = None
    r"""EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access EC2"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    username: Optional[str] = None
    r"""Username for Prometheus Basic authentication"""

    password: Optional[str] = None
    r"""Password for Prometheus Basic authentication"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    @field_serializer("discovery_type")
    def serialize_discovery_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputPrometheusDiscoveryType1(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthType2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("record_type")
    def serialize_record_type(self, value):
        if isinstance(value, str):
            try:
                return models.RecordTypeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("scrape_protocol")
    def serialize_scrape_protocol(self, value):
        if isinstance(value, str):
            try:
                return models.ScrapeProtocolOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AwsAuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


InputPrometheusTypedDict = TypeAliasType(
    "InputPrometheusTypedDict",
    Union[
        InputPrometheusPrometheus1TypedDict,
        InputPrometheusPrometheus2TypedDict,
        InputPrometheusPrometheus3TypedDict,
        InputPrometheusPrometheus4TypedDict,
        InputPrometheusPrometheus5TypedDict,
        InputPrometheusPrometheus6TypedDict,
        InputPrometheusPrometheus7TypedDict,
        InputPrometheusPrometheus8TypedDict,
        InputPrometheusPrometheus9TypedDict,
    ],
)


InputPrometheus = TypeAliasType(
    "InputPrometheus",
    Union[
        InputPrometheusPrometheus1,
        InputPrometheusPrometheus2,
        InputPrometheusPrometheus3,
        InputPrometheusPrometheus4,
        InputPrometheusPrometheus5,
        InputPrometheusPrometheus6,
        InputPrometheusPrometheus7,
        InputPrometheusPrometheus8,
        InputPrometheusPrometheus9,
    ],
)
