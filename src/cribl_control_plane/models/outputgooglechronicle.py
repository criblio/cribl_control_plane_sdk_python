"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .extrahttpheaderstype import ExtraHTTPHeadersType, ExtraHTTPHeadersTypeTypedDict
from .failedrequestloggingmodeoptions import FailedRequestLoggingModeOptions
from .metadatatype import MetadataType, MetadataTypeTypedDict
from .onbackpressureoptions import OnBackpressureOptions
from .pqcompressoptions import PqCompressOptions
from .pqmodeoptions import PqModeOptions
from .pqonbackpressureoptions import PqOnBackpressureOptions
from .responseretrysettingstype import (
    ResponseRetrySettingsType,
    ResponseRetrySettingsTypeTypedDict,
)
from .tagstype import TagsType, TagsTypeTypedDict
from .timeoutretrysettingstype import (
    TimeoutRetrySettingsType,
    TimeoutRetrySettingsTypeTypedDict,
)
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class OutputGoogleChronicleType10(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion10(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class OutputGoogleChronicleAuthenticationMethod10(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class SendEventsAs10(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType10TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType10(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType10(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle10TypedDict(TypedDict):
    type: OutputGoogleChronicleType10
    pq_controls: MetadataTypeTypedDict
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion10]
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod10]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs10]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType10TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType10]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""


class OutputGoogleChronicleGoogleChronicle10(BaseModel):
    type: OutputGoogleChronicleType10

    pq_controls: Annotated[MetadataType, pydantic.Field(alias="pqControls")]

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion10],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion10.V1

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod10],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod10.SERVICE_ACCOUNT

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs10], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs10.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType10]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType10], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType10.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion10(value)
            except ValueError:
                return value
        return value

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod10(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs10(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType10(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleType9(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion9(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class OutputGoogleChronicleAuthenticationMethod9(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class SendEventsAs9(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType9TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType9(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType9(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle9TypedDict(TypedDict):
    type: OutputGoogleChronicleType9
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion9]
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod9]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs9]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType9TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType9]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle9(BaseModel):
    type: OutputGoogleChronicleType9

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion9],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion9.V1

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod9],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod9.SERVICE_ACCOUNT

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs9], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs9.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType9]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType9], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType9.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion9(value)
            except ValueError:
                return value
        return value

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod9(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs9(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType9(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleAPIVersion8(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class OutputGoogleChronicleType8(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAuthenticationMethod8(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class SendEventsAs8(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType8TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType8(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType8(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle8TypedDict(TypedDict):
    type: OutputGoogleChronicleType8
    api_version: NotRequired[OutputGoogleChronicleAPIVersion8]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod8]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs8]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType8TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType8]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle8(BaseModel):
    type: OutputGoogleChronicleType8

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion8],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion8.V1

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod8],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod8.SERVICE_ACCOUNT

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs8], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs8.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType8]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType8], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType8.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion8(value)
            except ValueError:
                return value
        return value

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod8(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs8(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType8(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleAPIVersion7(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class OutputGoogleChronicleType7(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAuthenticationMethod7(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class SendEventsAs7(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType7TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType7(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType7(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle7TypedDict(TypedDict):
    type: OutputGoogleChronicleType7
    api_version: NotRequired[OutputGoogleChronicleAPIVersion7]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod7]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs7]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType7TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType7]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle7(BaseModel):
    type: OutputGoogleChronicleType7

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion7],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion7.V1

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod7],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod7.SERVICE_ACCOUNT

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs7], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs7.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType7]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType7], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType7.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion7(value)
            except ValueError:
                return value
        return value

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod7(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs7(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType7(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleAuthenticationMethod6(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class OutputGoogleChronicleType6(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion6(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class SendEventsAs6(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType6TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType6(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType6(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle6TypedDict(TypedDict):
    type: OutputGoogleChronicleType6
    service_account_credentials_secret: str
    r"""Select or create a stored text secret"""
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod6]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion6]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs6]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType6TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType6]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle6(BaseModel):
    type: OutputGoogleChronicleType6

    service_account_credentials_secret: Annotated[
        str, pydantic.Field(alias="serviceAccountCredentialsSecret")
    ]
    r"""Select or create a stored text secret"""

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod6],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod6.SERVICE_ACCOUNT

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion6],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion6.V1

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs6], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs6.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType6]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType6], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType6.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod6(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion6(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs6(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType6(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleAuthenticationMethod5(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class OutputGoogleChronicleType5(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion5(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class SendEventsAs5(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType5TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType5(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle5TypedDict(TypedDict):
    type: OutputGoogleChronicleType5
    service_account_credentials: str
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod5]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion5]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs5]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType5TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType5]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle5(BaseModel):
    type: OutputGoogleChronicleType5

    service_account_credentials: Annotated[
        str, pydantic.Field(alias="serviceAccountCredentials")
    ]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod5],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod5.SERVICE_ACCOUNT

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion5],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion5.V1

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs5], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs5.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType5]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType5], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType5.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod5(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion5(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs5(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType5(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleAuthenticationMethod4(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class OutputGoogleChronicleType4(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion4(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class SendEventsAs4(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType4TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType4(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle4TypedDict(TypedDict):
    type: OutputGoogleChronicleType4
    api_key_secret: str
    r"""Select or create a stored text secret"""
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod4]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion4]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs4]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType4TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType4]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle4(BaseModel):
    type: OutputGoogleChronicleType4

    api_key_secret: Annotated[str, pydantic.Field(alias="apiKeySecret")]
    r"""Select or create a stored text secret"""

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod4],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod4.SERVICE_ACCOUNT

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion4],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion4.V1

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs4], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs4.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType4]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType4], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType4.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod4(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion4(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs4(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType4(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class OutputGoogleChronicleAuthenticationMethod3(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class OutputGoogleChronicleType3(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion3(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class SendEventsAs3(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class ExtraLogType3TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType3(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle3TypedDict(TypedDict):
    type: OutputGoogleChronicleType3
    api_key: str
    r"""Organization's API key in Google SecOps"""
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod3]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion3]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    log_format_type: NotRequired[SendEventsAs3]
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType3TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType3]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle3(BaseModel):
    type: OutputGoogleChronicleType3

    api_key: Annotated[str, pydantic.Field(alias="apiKey")]
    r"""Organization's API key in Google SecOps"""

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod3],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod3.SERVICE_ACCOUNT

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion3],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion3.V1

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs3], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs3.UNSTRUCTURED

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType3]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType3], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType3.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod3(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion3(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs3(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType3(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class SendEventsAs2(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class OutputGoogleChronicleType2(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion2(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class OutputGoogleChronicleAuthenticationMethod2(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class ExtraLogType2TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType2(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle2TypedDict(TypedDict):
    type: OutputGoogleChronicleType2
    log_format_type: NotRequired[SendEventsAs2]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion2]
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod2]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    extra_log_types: NotRequired[List[ExtraLogType2TypedDict]]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: NotRequired[str]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: NotRequired[str]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: NotRequired[str]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: NotRequired[str]
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: NotRequired[List[TagsTypeTypedDict]]
    r"""Custom labels to be added to every batch"""
    udm_type: NotRequired[UDMType2]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle2(BaseModel):
    type: OutputGoogleChronicleType2

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs2], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs2.UNSTRUCTURED

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion2.V1

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod2.SERVICE_ACCOUNT

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    extra_log_types: Annotated[
        Optional[List[ExtraLogType2]], pydantic.Field(alias="extraLogTypes")
    ] = None
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[Optional[str], pydantic.Field(alias="logType")] = None
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[Optional[str], pydantic.Field(alias="logTextField")] = (
        None
    )
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[Optional[str], pydantic.Field(alias="customerId")] = None
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: Optional[str] = None
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[
        Optional[List[TagsType]], pydantic.Field(alias="customLabels")
    ] = None
    r"""Custom labels to be added to every batch"""

    udm_type: Annotated[
        Annotated[Optional[UDMType2], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType2.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs2(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion2(value)
            except ValueError:
                return value
        return value

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod2(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType2(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


class SendEventsAs1(str, Enum, metaclass=utils.OpenEnumMeta):
    # Unstructured
    UNSTRUCTURED = "unstructured"
    # UDM
    UDM = "udm"


class OutputGoogleChronicleType1(str, Enum):
    GOOGLE_CHRONICLE = "google_chronicle"


class OutputGoogleChronicleAPIVersion1(str, Enum, metaclass=utils.OpenEnumMeta):
    # V1
    V1 = "v1"
    # V2
    V2 = "v2"


class OutputGoogleChronicleAuthenticationMethod1(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # API key
    MANUAL = "manual"
    # API key secret
    SECRET = "secret"
    # Service account credentials
    SERVICE_ACCOUNT = "serviceAccount"
    # Service account credentials secret
    SERVICE_ACCOUNT_SECRET = "serviceAccountSecret"


class ExtraLogType1TypedDict(TypedDict):
    log_type: str
    description: NotRequired[str]


class ExtraLogType1(BaseModel):
    log_type: Annotated[str, pydantic.Field(alias="logType")]

    description: Optional[str] = None


class UDMType1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    ENTITIES = "entities"
    LOGS = "logs"


class OutputGoogleChronicleGoogleChronicle1TypedDict(TypedDict):
    type: OutputGoogleChronicleType1
    extra_log_types: List[ExtraLogType1TypedDict]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""
    log_type: str
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""
    log_text_field: str
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""
    customer_id: str
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""
    namespace: str
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""
    custom_labels: List[TagsTypeTypedDict]
    r"""Custom labels to be added to every batch"""
    log_format_type: NotRequired[SendEventsAs1]
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    api_version: NotRequired[OutputGoogleChronicleAPIVersion1]
    authentication_method: NotRequired[OutputGoogleChronicleAuthenticationMethod1]
    response_retry_settings: NotRequired[List[ResponseRetrySettingsTypeTypedDict]]
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""
    timeout_retry_settings: NotRequired[TimeoutRetrySettingsTypeTypedDict]
    response_honor_retry_after_header: NotRequired[bool]
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""
    region: NotRequired[str]
    r"""Regional endpoint to send events to"""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing requests before blocking"""
    max_payload_size_kb: NotRequired[float]
    r"""Maximum size, in KB, of the request body"""
    max_payload_events: NotRequired[float]
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""
    compress: NotRequired[bool]
    r"""Compress the payload body before sending"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """
    timeout_sec: NotRequired[float]
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""
    extra_http_headers: NotRequired[List[ExtraHTTPHeadersTypeTypedDict]]
    r"""Headers to add to all events"""
    failed_request_logging_mode: NotRequired[FailedRequestLoggingModeOptions]
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""
    safe_headers: NotRequired[List[str]]
    r"""List of headers that are safe to log in plain text"""
    use_round_robin_dns: NotRequired[bool]
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""
    on_backpressure: NotRequired[OnBackpressureOptions]
    r"""How to handle events when all receivers are exerting backpressure"""
    total_memory_limit_kb: NotRequired[float]
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""
    description: NotRequired[str]
    udm_type: NotRequired[UDMType1]
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""
    api_key: NotRequired[str]
    r"""Organization's API key in Google SecOps"""
    api_key_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    service_account_credentials: NotRequired[str]
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""
    service_account_credentials_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    pq_strict_ordering: NotRequired[bool]
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""
    pq_rate_per_sec: NotRequired[float]
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""
    pq_mode: NotRequired[PqModeOptions]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    pq_max_backpressure_sec: NotRequired[float]
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_controls: NotRequired[MetadataTypeTypedDict]


class OutputGoogleChronicleGoogleChronicle1(BaseModel):
    type: OutputGoogleChronicleType1

    extra_log_types: Annotated[
        List[ExtraLogType1], pydantic.Field(alias="extraLogTypes")
    ]
    r"""Custom log types. If the value \"Custom\" is selected in the setting \"Default log type\" above, the first custom log type in this table will be automatically selected as default log type."""

    log_type: Annotated[str, pydantic.Field(alias="logType")]
    r"""Default log type value to send to SecOps. Can be overwritten by event field __logType."""

    log_text_field: Annotated[str, pydantic.Field(alias="logTextField")]
    r"""Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event."""

    customer_id: Annotated[str, pydantic.Field(alias="customerId")]
    r"""A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication."""

    namespace: str
    r"""User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace."""

    custom_labels: Annotated[List[TagsType], pydantic.Field(alias="customLabels")]
    r"""Custom labels to be added to every batch"""

    log_format_type: Annotated[
        Annotated[Optional[SendEventsAs1], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="logFormatType"),
    ] = SendEventsAs1.UNSTRUCTURED

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    api_version: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAPIVersion1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="apiVersion"),
    ] = OutputGoogleChronicleAPIVersion1.V1

    authentication_method: Annotated[
        Annotated[
            Optional[OutputGoogleChronicleAuthenticationMethod1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authenticationMethod"),
    ] = OutputGoogleChronicleAuthenticationMethod1.SERVICE_ACCOUNT

    response_retry_settings: Annotated[
        Optional[List[ResponseRetrySettingsType]],
        pydantic.Field(alias="responseRetrySettings"),
    ] = None
    r"""Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)"""

    timeout_retry_settings: Annotated[
        Optional[TimeoutRetrySettingsType], pydantic.Field(alias="timeoutRetrySettings")
    ] = None

    response_honor_retry_after_header: Annotated[
        Optional[bool], pydantic.Field(alias="responseHonorRetryAfterHeader")
    ] = False
    r"""Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored."""

    region: Optional[str] = None
    r"""Regional endpoint to send events to"""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing requests before blocking"""

    max_payload_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadSizeKB")
    ] = 1024
    r"""Maximum size, in KB, of the request body"""

    max_payload_events: Annotated[
        Optional[float], pydantic.Field(alias="maxPayloadEvents")
    ] = 0
    r"""Maximum number of events to include in the request body. Default is 0 (unlimited)."""

    compress: Optional[bool] = True
    r"""Compress the payload body before sending"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
    Enabled by default. When this setting is also present in TLS Settings (Client Side),
    that value will take precedence.
    """

    timeout_sec: Annotated[Optional[float], pydantic.Field(alias="timeoutSec")] = 90
    r"""Amount of time, in seconds, to wait for a request to complete before canceling it"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit."""

    extra_http_headers: Annotated[
        Optional[List[ExtraHTTPHeadersType]], pydantic.Field(alias="extraHttpHeaders")
    ] = None
    r"""Headers to add to all events"""

    failed_request_logging_mode: Annotated[
        Annotated[
            Optional[FailedRequestLoggingModeOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="failedRequestLoggingMode"),
    ] = FailedRequestLoggingModeOptions.NONE
    r"""Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below."""

    safe_headers: Annotated[
        Optional[List[str]], pydantic.Field(alias="safeHeaders")
    ] = None
    r"""List of headers that are safe to log in plain text"""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = False
    r"""Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OnBackpressureOptions.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    total_memory_limit_kb: Annotated[
        Optional[float], pydantic.Field(alias="totalMemoryLimitKB")
    ] = None
    r"""Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced."""

    description: Optional[str] = None

    udm_type: Annotated[
        Annotated[Optional[UDMType1], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="udmType"),
    ] = UDMType1.LOGS
    r"""Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent."""

    api_key: Annotated[Optional[str], pydantic.Field(alias="apiKey")] = None
    r"""Organization's API key in Google SecOps"""

    api_key_secret: Annotated[Optional[str], pydantic.Field(alias="apiKeySecret")] = (
        None
    )
    r"""Select or create a stored text secret"""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right."""

    service_account_credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentialsSecret")
    ] = None
    r"""Select or create a stored text secret"""

    pq_strict_ordering: Annotated[
        Optional[bool], pydantic.Field(alias="pqStrictOrdering")
    ] = True
    r"""Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed."""

    pq_rate_per_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqRatePerSec")
    ] = 0
    r"""Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling."""

    pq_mode: Annotated[
        Annotated[Optional[PqModeOptions], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="pqMode"),
    ] = PqModeOptions.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBufferSize")
    ] = 42
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    pq_max_backpressure_sec: Annotated[
        Optional[float], pydantic.Field(alias="pqMaxBackpressureSec")
    ] = 30
    r"""How long (in seconds) to wait for backpressure to resolve before engaging the queue"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqCompress"),
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_controls: Annotated[
        Optional[MetadataType], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("log_format_type")
    def serialize_log_format_type(self, value):
        if isinstance(value, str):
            try:
                return models.SendEventsAs1(value)
            except ValueError:
                return value
        return value

    @field_serializer("api_version")
    def serialize_api_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAPIVersion1(value)
            except ValueError:
                return value
        return value

    @field_serializer("authentication_method")
    def serialize_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputGoogleChronicleAuthenticationMethod1(value)
            except ValueError:
                return value
        return value

    @field_serializer("failed_request_logging_mode")
    def serialize_failed_request_logging_mode(self, value):
        if isinstance(value, str):
            try:
                return models.FailedRequestLoggingModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OnBackpressureOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("udm_type")
    def serialize_udm_type(self, value):
        if isinstance(value, str):
            try:
                return models.UDMType1(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_mode")
    def serialize_pq_mode(self, value):
        if isinstance(value, str):
            try:
                return models.PqModeOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_compress")
    def serialize_pq_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("pq_on_backpressure")
    def serialize_pq_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value


OutputGoogleChronicleTypedDict = TypeAliasType(
    "OutputGoogleChronicleTypedDict",
    Union[
        OutputGoogleChronicleGoogleChronicle1TypedDict,
        OutputGoogleChronicleGoogleChronicle2TypedDict,
        OutputGoogleChronicleGoogleChronicle3TypedDict,
        OutputGoogleChronicleGoogleChronicle4TypedDict,
        OutputGoogleChronicleGoogleChronicle5TypedDict,
        OutputGoogleChronicleGoogleChronicle6TypedDict,
        OutputGoogleChronicleGoogleChronicle7TypedDict,
        OutputGoogleChronicleGoogleChronicle8TypedDict,
        OutputGoogleChronicleGoogleChronicle9TypedDict,
        OutputGoogleChronicleGoogleChronicle10TypedDict,
    ],
)


OutputGoogleChronicle = TypeAliasType(
    "OutputGoogleChronicle",
    Union[
        OutputGoogleChronicleGoogleChronicle1,
        OutputGoogleChronicleGoogleChronicle2,
        OutputGoogleChronicleGoogleChronicle3,
        OutputGoogleChronicleGoogleChronicle4,
        OutputGoogleChronicleGoogleChronicle5,
        OutputGoogleChronicleGoogleChronicle6,
        OutputGoogleChronicleGoogleChronicle7,
        OutputGoogleChronicleGoogleChronicle8,
        OutputGoogleChronicleGoogleChronicle9,
        OutputGoogleChronicleGoogleChronicle10,
    ],
)
