"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .format2options import Format2Options
from .pqcompressoptions import PqCompressOptions
from .pqonbackpressureoptions import PqOnBackpressureOptions
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputRingType(str, Enum):
    RING = "ring"


class OutputRingTypedDict(TypedDict):
    type: OutputRingType
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    format_: NotRequired[Format2Options]
    r"""Format to use to serialize events before writing to the Event Hubs Kafka brokers"""
    partition_expr: NotRequired[str]
    r"""JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition."""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[PqCompressOptions]
    r"""Codec to use to compress the persisted data"""
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>"""
    on_backpressure: NotRequired[PqOnBackpressureOptions]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    description: NotRequired[str]


class OutputRing(BaseModel):
    type: OutputRingType

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    format_: Annotated[
        Annotated[Optional[Format2Options], PlainValidator(validate_open_enum(False))],
        pydantic.Field(alias="format"),
    ] = Format2Options.JSON
    r"""Format to use to serialize events before writing to the Event Hubs Kafka brokers"""

    partition_expr: Annotated[Optional[str], pydantic.Field(alias="partitionExpr")] = (
        None
    )
    r"""JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition."""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Annotated[
        Optional[PqCompressOptions], PlainValidator(validate_open_enum(False))
    ] = PqCompressOptions.NONE
    r"""Codec to use to compress the persisted data"""

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = None
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>"""

    on_backpressure: Annotated[
        Annotated[
            Optional[PqOnBackpressureOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = PqOnBackpressureOptions.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    description: Optional[str] = None

    @field_serializer("format_")
    def serialize_format_(self, value):
        if isinstance(value, str):
            try:
                return models.Format2Options(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.PqCompressOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.PqOnBackpressureOptions(value)
            except ValueError:
                return value
        return value
