"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .executortype import ExecutorType, ExecutorTypeTypedDict
from .scheduletype import ScheduleType, ScheduleTypeTypedDict
from .type1options import Type1Options
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class RunnableJobExecutorLogLevel(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Level at which to set task logging"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"
    SILLY = "silly"


class RunnableJobExecutorRunTypedDict(TypedDict):
    reschedule_dropped_tasks: NotRequired[bool]
    r"""Reschedule tasks that failed with non-fatal errors"""
    max_task_reschedule: NotRequired[float]
    r"""Maximum number of times a task can be rescheduled"""
    log_level: NotRequired[RunnableJobExecutorLogLevel]
    r"""Level at which to set task logging"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time."""


class RunnableJobExecutorRun(BaseModel):
    reschedule_dropped_tasks: Annotated[
        Optional[bool], pydantic.Field(alias="rescheduleDroppedTasks")
    ] = True
    r"""Reschedule tasks that failed with non-fatal errors"""

    max_task_reschedule: Annotated[
        Optional[float], pydantic.Field(alias="maxTaskReschedule")
    ] = 1
    r"""Maximum number of times a task can be rescheduled"""

    log_level: Annotated[
        Annotated[
            Optional[RunnableJobExecutorLogLevel],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="logLevel"),
    ] = RunnableJobExecutorLogLevel.INFO
    r"""Level at which to set task logging"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = "0"
    r"""Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time."""

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.RunnableJobExecutorLogLevel(value)
            except ValueError:
                return value
        return value


class RunnableJobExecutorTypedDict(TypedDict):
    executor: ExecutorTypeTypedDict
    run: RunnableJobExecutorRunTypedDict
    id: NotRequired[str]
    r"""Unique ID for this Job"""
    description: NotRequired[str]
    type: NotRequired[Type1Options]
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    remove_fields: NotRequired[List[str]]
    r"""List of fields to remove from Discover results. Wildcards (for example, aws*) are allowed. This is useful when discovery returns sensitive fields that should not be exposed in the Jobs user interface."""
    resume_on_boot: NotRequired[bool]
    r"""Resume the ad hoc job if a failure condition causes Stream to restart during job execution"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    schedule: NotRequired[ScheduleTypeTypedDict]
    r"""Configuration for a scheduled job"""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""


class RunnableJobExecutor(BaseModel):
    executor: ExecutorType

    run: RunnableJobExecutorRun

    id: Optional[str] = None
    r"""Unique ID for this Job"""

    description: Optional[str] = None

    type: Annotated[
        Optional[Type1Options], PlainValidator(validate_open_enum(False))
    ] = None

    ttl: Optional[str] = "4h"
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = False
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    remove_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="removeFields")
    ] = None
    r"""List of fields to remove from Discover results. Wildcards (for example, aws*) are allowed. This is useful when discovery returns sensitive fields that should not be exposed in the Jobs user interface."""

    resume_on_boot: Annotated[Optional[bool], pydantic.Field(alias="resumeOnBoot")] = (
        False
    )
    r"""Resume the ad hoc job if a failure condition causes Stream to restart during job execution"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    schedule: Optional[ScheduleType] = None
    r"""Configuration for a scheduled job"""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.Type1Options(value)
            except ValueError:
                return value
        return value
