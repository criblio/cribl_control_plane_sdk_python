"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authenticationmethodoptionss3collectorconf import (
    AuthenticationMethodOptionsS3CollectorConf,
)
from .signatureversionoptionss3collectorconf import (
    SignatureVersionOptionsS3CollectorConf,
)
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from cribl_control_plane.utils.unions import parse_open_union
from enum import Enum
from functools import partial
import pydantic
from pydantic import ConfigDict, field_serializer, model_serializer
from pydantic.functional_validators import BeforeValidator
from typing import Any, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class S3AwsAuthenticationMethodSecretPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3AwsAuthenticationMethodSecretExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodSecretExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodSecretTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[S3AwsAuthenticationMethodSecretPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3AwsAuthenticationMethodSecretExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptionsS3CollectorConf]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3AwsAuthenticationMethodSecret(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Optional[S3AwsAuthenticationMethodSecretPartitioningScheme],
        pydantic.Field(alias="partitioningScheme"),
    ] = None
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3AwsAuthenticationMethodSecretExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptionsS3CollectorConf],
        pydantic.Field(alias="signatureVersion"),
    ] = None
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = None
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = None
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodSecretPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "awsAuthenticationMethod",
                "awsSecret",
                "outputName",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "region",
                "path",
                "partitioningScheme",
                "extractors",
                "endpoint",
                "signatureVersion",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "maxBatchSize",
                "reuseConnections",
                "rejectUnauthorized",
                "verifyPermissions",
                "disableTimeFilter",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class S3AwsAuthenticationMethodManualPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3AwsAuthenticationMethodManualExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodManualExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodManualTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[S3AwsAuthenticationMethodManualPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3AwsAuthenticationMethodManualExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptionsS3CollectorConf]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3AwsAuthenticationMethodManual(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Optional[S3AwsAuthenticationMethodManualPartitioningScheme],
        pydantic.Field(alias="partitioningScheme"),
    ] = None
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3AwsAuthenticationMethodManualExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptionsS3CollectorConf],
        pydantic.Field(alias="signatureVersion"),
    ] = None
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = None
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = None
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodManualPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "awsAuthenticationMethod",
                "awsApiKey",
                "awsSecretKey",
                "outputName",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "region",
                "path",
                "partitioningScheme",
                "extractors",
                "endpoint",
                "signatureVersion",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "maxBatchSize",
                "reuseConnections",
                "rejectUnauthorized",
                "verifyPermissions",
                "disableTimeFilter",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class S3AwsAuthenticationMethodAutoPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3AwsAuthenticationMethodAutoExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodAutoExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodAutoTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[S3AwsAuthenticationMethodAutoPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3AwsAuthenticationMethodAutoExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptionsS3CollectorConf]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3AwsAuthenticationMethodAuto(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Optional[S3AwsAuthenticationMethodAutoPartitioningScheme],
        pydantic.Field(alias="partitioningScheme"),
    ] = None
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3AwsAuthenticationMethodAutoExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptionsS3CollectorConf],
        pydantic.Field(alias="signatureVersion"),
    ] = None
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = None
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = None
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodAutoPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "awsAuthenticationMethod",
                "outputName",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "region",
                "path",
                "partitioningScheme",
                "extractors",
                "endpoint",
                "signatureVersion",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "maxBatchSize",
                "reuseConnections",
                "rejectUnauthorized",
                "verifyPermissions",
                "disableTimeFilter",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class S3PartitioningSchemeNonePartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3PartitioningSchemeNoneExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeNoneExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeNoneTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    partitioning_scheme: NotRequired[S3PartitioningSchemeNonePartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    recurse: NotRequired[bool]
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    extractors: NotRequired[List[S3PartitioningSchemeNoneExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptionsS3CollectorConf]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3PartitioningSchemeNone(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    partitioning_scheme: Annotated[
        Optional[S3PartitioningSchemeNonePartitioningScheme],
        pydantic.Field(alias="partitioningScheme"),
    ] = None
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    recurse: Optional[bool] = None
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    extractors: Optional[List[S3PartitioningSchemeNoneExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptionsS3CollectorConf],
        pydantic.Field(alias="signatureVersion"),
    ] = None
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = None
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = None
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeNonePartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "partitioningScheme",
                "recurse",
                "outputName",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "region",
                "path",
                "extractors",
                "awsAuthenticationMethod",
                "endpoint",
                "signatureVersion",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "maxBatchSize",
                "reuseConnections",
                "rejectUnauthorized",
                "verifyPermissions",
                "disableTimeFilter",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class S3PartitioningSchemeDdssPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3PartitioningSchemeDdssExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeDdssExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeDdssTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    partitioning_scheme: NotRequired[S3PartitioningSchemeDdssPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    extractors: NotRequired[List[S3PartitioningSchemeDdssExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[AuthenticationMethodOptionsS3CollectorConf]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptionsS3CollectorConf]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3PartitioningSchemeDdss(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    partitioning_scheme: Annotated[
        Optional[S3PartitioningSchemeDdssPartitioningScheme],
        pydantic.Field(alias="partitioningScheme"),
    ] = None
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    extractors: Optional[List[S3PartitioningSchemeDdssExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Optional[AuthenticationMethodOptionsS3CollectorConf],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = None
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptionsS3CollectorConf],
        pydantic.Field(alias="signatureVersion"),
    ] = None
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = None
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = None
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeDdssPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "partitioningScheme",
                "outputName",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "region",
                "path",
                "extractors",
                "awsAuthenticationMethod",
                "endpoint",
                "signatureVersion",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "maxBatchSize",
                "reuseConnections",
                "rejectUnauthorized",
                "verifyPermissions",
                "disableTimeFilter",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


S3CollectorConfTypedDict = TypeAliasType(
    "S3CollectorConfTypedDict",
    Union[
        S3PartitioningSchemeDdssTypedDict,
        S3AwsAuthenticationMethodAutoTypedDict,
        S3PartitioningSchemeNoneTypedDict,
        S3AwsAuthenticationMethodSecretTypedDict,
        S3AwsAuthenticationMethodManualTypedDict,
    ],
)


class UnknownS3CollectorConf(BaseModel):
    r"""A S3CollectorConf variant the SDK doesn't recognize. Preserves the raw payload."""

    aws_authentication_method: Literal["UNKNOWN"] = "UNKNOWN"
    raw: Any
    is_unknown: Literal[True] = True

    model_config = ConfigDict(frozen=True)


_S3_COLLECTOR_CONF_VARIANTS: dict[str, Any] = {
    "auto": S3AwsAuthenticationMethodAuto,
    "manual": S3AwsAuthenticationMethodManual,
    "secret": S3AwsAuthenticationMethodSecret,
}


S3CollectorConf = Annotated[
    Union[
        S3AwsAuthenticationMethodAuto,
        S3AwsAuthenticationMethodManual,
        S3AwsAuthenticationMethodSecret,
        UnknownS3CollectorConf,
    ],
    BeforeValidator(
        partial(
            parse_open_union,
            disc_key="awsAuthenticationMethod",
            variants=_S3_COLLECTOR_CONF_VARIANTS,
            unknown_cls=UnknownS3CollectorConf,
            union_name="S3CollectorConf",
        )
    ),
]


try:
    S3AwsAuthenticationMethodSecret.model_rebuild()
except NameError:
    pass
try:
    S3AwsAuthenticationMethodManual.model_rebuild()
except NameError:
    pass
try:
    S3AwsAuthenticationMethodAuto.model_rebuild()
except NameError:
    pass
try:
    S3PartitioningSchemeNone.model_rebuild()
except NameError:
    pass
try:
    S3PartitioningSchemeDdss.model_rebuild()
except NameError:
    pass
